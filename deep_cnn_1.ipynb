{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_cnn_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oxygen0605/ImageClassification/blob/master/deep_cnn_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozlZDHeF512V",
        "colab_type": "text"
      },
      "source": [
        "# Google Colaboratory環境の初期設定\n",
        "\n",
        "## インスタンスのマシンスペック "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu5E0_Nm05m-",
        "colab_type": "code",
        "outputId": "2f28b338-3552-4d61-e683-9cbfb0c41cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!nvidia-smi\n",
        "!cat /proc/driver/nvidia/gpus/0000:00:04.0/information\n",
        "!cat /etc/issue\n",
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Aug 18 03:56:43 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    38W /  70W |   7769MiB / 15079MiB |     73%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Ubuntu 18.04.2 LTS \\n \\l\n",
            "\n",
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "Model: \t\t Tesla T4\n",
            "IRQ:   \t\t 11\n",
            "GPU UUID: \t GPU-b9cc0e36-2f02-6940-bf79-b92b4e1062cc\n",
            "Video BIOS: \t 90.04.21.00.01\n",
            "Bus Type: \t PCI\n",
            "DMA Size: \t 47 bits\n",
            "DMA Mask: \t 0x7fffffffffff\n",
            "Bus Location: \t 0000:00:04.0\n",
            "Device Minor: \t 0\n",
            "Blacklisted:\t No\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLk9BEEX7PVD",
        "colab_type": "text"
      },
      "source": [
        "# Google Driveにマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE2RmNVd2bA9",
        "colab_type": "code",
        "outputId": "3d293fd3-8e17-4369-cbca-b70c531728de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45yQPr6Lltrj",
        "colab_type": "text"
      },
      "source": [
        "# Deep CNN (CIFAR-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur3z5pdimJdX",
        "colab_type": "text"
      },
      "source": [
        "## モデルの生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CLdMi7Wlzwr",
        "colab_type": "code",
        "outputId": "dd9e9225-dbc8-45fc-87bd-36b57d807be4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Dense, BatchNormalization\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Activation, Flatten\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras import regularizers\n",
        "\n",
        "def deep_cnn(input_shape, num_classes):\n",
        "    inputs = Input(shape = input_shape)\n",
        "    \n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(inputs)\n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(512,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(512,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = Dense(1024,activation = \"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1024,activation = \"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    y  = Dense(num_classes, activation = \"softmax\")(x)\n",
        "\n",
        "    return Model(input = inputs, output = y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygmn8vNImOV-",
        "colab_type": "text"
      },
      "source": [
        "## CIFAR10 データセットの用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JELm-0dimTvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "class CIFAR10Dataset():\n",
        "\tdef __init__(self):\n",
        "\t\tself.image_shape = (32, 32, 3)\n",
        "\t\tself.num_classes = 10\n",
        "\t\t\n",
        "\tdef preprocess(self, data, label_data=False):\n",
        "\t\tif label_data:\n",
        "\t\t\t# conver class number to one-hot vector\n",
        "\t\t\tdata = keras.utils.to_categorical(data, self.num_classes)\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tdata = data.astype(\"float32\")\n",
        "\t\t\tdata /= 255 #convert the value to 0 ~ 1 scale\n",
        "\t\t\tshape = (data.shape[0],) + self.image_shape\n",
        "\t\t\tdata = data.reshape(shape)\n",
        "\t\t\t\n",
        "\t\treturn data\n",
        "\t\n",
        "\tdef get_batch(self):\n",
        "\t\t# x: data, y: lebel\n",
        "\t\t(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\t\t\n",
        "\t\tx_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
        "\t\ty_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
        "\t\t\t\t\t [y_train, y_test]]\n",
        "\t\t\n",
        "\t\treturn x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TWZvmmxmsK2",
        "colab_type": "text"
      },
      "source": [
        "## Training, Evaluation用クラスの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxj9ZLVbmqnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
        "from keras.optimizers import RMSprop, Adam, Nadam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "class Trainer():\n",
        "\t\n",
        "\tdef __init__(self, model, loss, optimizer, logdir = './'):\n",
        "\t\tself._target = model\n",
        "\t\tself._target.compile(\n",
        "\t\t\t\tloss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
        "\t\t\t\t)\n",
        "\t\tself.verbose = 1 # visualize progress bar: 0(OFF), 1(On), 2(On:each data) \n",
        "\t\t#self.log_dir = os.path.join(os.path.dirname(__file__), logdir)\n",
        "\t\tself.log_dir = os.path.join(logdir)\n",
        "\t\tself.model_file_name = \"model_file.hdf5\"\n",
        "\t\n",
        "\tdef train_with_data_augmentation(self, \n",
        "            x_train, y_train, batch_size, epochs, validation_split, lr_scheduler):\n",
        "\t\t\"\"\"\n",
        "        fit(ImageDataGeneratorクラス)\n",
        "        与えられたサンプルデータに基づいて，データに依存する統計量を計算します． \n",
        "        featurewise_center，featurewise_std_normalization，\n",
        "        または，zca_whiteningが指定されたときに必要です．\n",
        "        \n",
        "        fit_generator(Modelクラス)\n",
        "         Pythonジェネレータ（またはSequenceのインスタンス）によりバッチ毎に生成されたデータでモデルを訓練します．\n",
        "         本ジェネレータは効率性のためモデルに並列して実行されます．例えば，モデルをGPUで学習させながら\n",
        "         CPU上で画像のリアルタイムデータ拡張を行うことができるようになります．\n",
        "        \"\"\"\n",
        "\t\t\"\"\"\n",
        "\t\t# remove previous execution\n",
        "\t\tif os.path.exists(self.log_dir):\n",
        "\t\t\timport shutil\n",
        "\t\t\tshutil.rmtree(self.log_dir) \n",
        "\t\tos.mkdir(self.log_dir)\n",
        "\t\t\"\"\"\n",
        "        \n",
        "\t\tdatagen = ImageDataGenerator(\n",
        "\t\t\tfeaturewise_center=False,                  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,             # set each sample mean to 0\n",
        "            featurewise_std_normalization=False, # divide inputs by std\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,                 # apply ZCA whitening\n",
        "            rotation_range=20,                   # randomly rotate images in the range (0~180)\n",
        "            width_shift_range=0.2,               # randomly shift images horizontally\n",
        "            height_shift_range=0.2,              # randomly shift images vertically\n",
        "            zoom_range = 0.2,\n",
        "            channel_shift_range = 0.2,\n",
        "            horizontal_flip=True,                # randomly flip images\n",
        "            vertical_flip=False                  # randomly flip images\n",
        "\t\t)\n",
        "\t\t\n",
        "    # compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied)\n",
        "\t\tdatagen.fit(x_train)\n",
        "\t\t\n",
        "    # for reproducibility\n",
        "\t\tnp.random.seed(1671)\n",
        "    #split for validation data\n",
        "\t\tindices = np.arange(x_train.shape[0])\n",
        "\t\tnp.random.shuffle(indices)\n",
        "\t\t\n",
        "\t\t#シャッフルしたindicesの後ろからvalidataion_size分だけvalidationに回す\n",
        "\t\tvalidation_size = int(x_train.shape[0] * validation_split)\n",
        "\t\tx_train, x_valid = \\\n",
        "            x_train[indices[:-validation_size], :], \\\n",
        "            x_train[indices[-validation_size:], :]\n",
        "\t\ty_train, y_valid = \\\n",
        "            y_train[indices[:-validation_size], :], \\\n",
        "            y_train[indices[-validation_size:], :]\n",
        "        \n",
        "    # training        \n",
        "\t\tmodel_path = os.path.join(self.log_dir, self.model_file_name)\n",
        "\t\tself._target.fit_generator(\n",
        "            datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "            steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_valid, y_valid),\n",
        "            callbacks=[\n",
        "                LearningRateScheduler(lr_scheduler),\n",
        "                TensorBoard(log_dir=self.log_dir),\n",
        "                ModelCheckpoint(model_path, save_best_only=True)\n",
        "            ],\n",
        "            verbose=self.verbose,\n",
        "            workers=4\n",
        "        )\n",
        "\t\t\n",
        "\n",
        "class Evaluator():\n",
        "    \n",
        "    def __init__(self, result_file_path=\"./prediction_result.csv\"):\n",
        "        self.result_file_path=\"./prediction_result.csv\"\n",
        "        \n",
        "    def simple_evaluate(self, model, x_test, label):\n",
        "        print(\"start evaluation...\")\n",
        "        score = model.evaluate(x_test, y_test, verbose=1)\n",
        "        print(\"Test loss:\", score[0])\n",
        "        print(\"Test accuracy:\", score[1])\n",
        "    \n",
        "    def tta_evaluate(self, model, x_test, batch_size = 2500, tta_epochs = 2):\n",
        "        print(\"batch size (TTA): \"+str(batch_size))\n",
        "        print(\"epochs (TTA): \"+str(tta_epochs))\n",
        "        tta = TTA()\n",
        "        tta_pred = tta.predict(model, x_test, batch_size, epochs = tta_epochs)\n",
        "        print(\"Test accuracy(TTA): \",end = \"\")\n",
        "        print( accuracy_score( np.argmax(tta_pred,axis = 1) , np.argmax(y_test,axis = 1)))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCiJEpaB-HAv",
        "colab_type": "text"
      },
      "source": [
        "## 学習率減衰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4me_PZO-F2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_rate_schedule_for_Adam(epoch):\n",
        "\tlr = 0.001\n",
        "\tif(epoch >= 100): lr = 0.0002 #100\n",
        "\tif(epoch >= 140): lr = 0.0001 #140\n",
        "\treturn lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKULnAcX2ilW",
        "colab_type": "text"
      },
      "source": [
        "## 実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzvig0oHnqoY",
        "colab_type": "code",
        "outputId": "20b783a3-c72f-436a-fd9e-1247c277f783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# create dataset\n",
        "dataset = CIFAR10Dataset()\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "\n",
        "# create model\n",
        "model = deep_cnn(dataset.image_shape, dataset.num_classes)\n",
        "\n",
        "# train the model\n",
        "# RMSprpの方がいいかもしれない\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=Adam())\n",
        "#trainer.simple_train(x_train, y_train, batch_size=500, epochs=10, validation_split=0.2)\n",
        "trainer.train_with_data_augmentation(\n",
        "          x_train, y_train, batch_size=500, epochs=150, validation_split=0.2, \n",
        "          lr_scheduler=learning_rate_schedule_for_Adam)\n",
        "\n",
        "# show result\n",
        "evaluator = Evaluator()\n",
        "score = evaluator.simple_evaluate(model, x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "W0817 19:05:53.672603 140310145603456 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0817 19:05:58.751287 140310145603456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0817 19:05:58.753152 140310145603456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "80/80 [==============================] - 55s 686ms/step - loss: 1.9567 - acc: 0.2368 - val_loss: 5.3625 - val_acc: 0.1634\n",
            "Epoch 2/150\n",
            "80/80 [==============================] - 37s 463ms/step - loss: 1.6568 - acc: 0.3676 - val_loss: 2.5036 - val_acc: 0.2863\n",
            "Epoch 3/150\n",
            "80/80 [==============================] - 37s 462ms/step - loss: 1.4858 - acc: 0.4505 - val_loss: 1.9913 - val_acc: 0.4123\n",
            "Epoch 4/150\n",
            "80/80 [==============================] - 38s 473ms/step - loss: 1.3447 - acc: 0.5136 - val_loss: 3.0249 - val_acc: 0.3684\n",
            "Epoch 5/150\n",
            "80/80 [==============================] - 37s 464ms/step - loss: 1.2415 - acc: 0.5575 - val_loss: 1.7186 - val_acc: 0.4914\n",
            "Epoch 6/150\n",
            "80/80 [==============================] - 36s 447ms/step - loss: 1.0870 - acc: 0.6125 - val_loss: 1.0276 - val_acc: 0.6331\n",
            "Epoch 7/150\n",
            "80/80 [==============================] - 37s 463ms/step - loss: 1.0317 - acc: 0.6307 - val_loss: 1.0277 - val_acc: 0.6392\n",
            "Epoch 8/150\n",
            "80/80 [==============================] - 37s 461ms/step - loss: 0.9928 - acc: 0.6472 - val_loss: 0.9171 - val_acc: 0.6695\n",
            "Epoch 9/150\n",
            "80/80 [==============================] - 36s 451ms/step - loss: 0.9703 - acc: 0.6567 - val_loss: 0.9859 - val_acc: 0.6529\n",
            "Epoch 10/150\n",
            "80/80 [==============================] - 37s 457ms/step - loss: 0.9423 - acc: 0.6662 - val_loss: 0.8677 - val_acc: 0.6902\n",
            "Epoch 11/150\n",
            "80/80 [==============================] - 36s 448ms/step - loss: 0.9162 - acc: 0.6781 - val_loss: 0.8948 - val_acc: 0.6783\n",
            "Epoch 12/150\n",
            "80/80 [==============================] - 36s 453ms/step - loss: 0.8917 - acc: 0.6849 - val_loss: 0.8732 - val_acc: 0.7024\n",
            "Epoch 13/150\n",
            "80/80 [==============================] - 36s 446ms/step - loss: 0.8742 - acc: 0.6944 - val_loss: 0.7465 - val_acc: 0.7382\n",
            "Epoch 14/150\n",
            "80/80 [==============================] - 25s 315ms/step - loss: 0.8556 - acc: 0.6997 - val_loss: 0.7600 - val_acc: 0.7281\n",
            "Epoch 15/150\n",
            "80/80 [==============================] - 26s 322ms/step - loss: 0.8296 - acc: 0.7107 - val_loss: 0.8587 - val_acc: 0.7003\n",
            "Epoch 16/150\n",
            "80/80 [==============================] - 26s 323ms/step - loss: 0.7932 - acc: 0.7258 - val_loss: 0.7465 - val_acc: 0.7342\n",
            "Epoch 17/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.7764 - acc: 0.7333 - val_loss: 0.8067 - val_acc: 0.7326\n",
            "Epoch 18/150\n",
            "80/80 [==============================] - 26s 320ms/step - loss: 0.7684 - acc: 0.7331 - val_loss: 0.8724 - val_acc: 0.7182\n",
            "Epoch 19/150\n",
            "80/80 [==============================] - 26s 323ms/step - loss: 0.7550 - acc: 0.7388 - val_loss: 0.6696 - val_acc: 0.7679\n",
            "Epoch 20/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.7412 - acc: 0.7448 - val_loss: 0.7072 - val_acc: 0.7578\n",
            "Epoch 21/150\n",
            "80/80 [==============================] - 26s 323ms/step - loss: 0.7294 - acc: 0.7487 - val_loss: 0.6925 - val_acc: 0.7635\n",
            "Epoch 22/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.7200 - acc: 0.7491 - val_loss: 0.6751 - val_acc: 0.7700\n",
            "Epoch 23/150\n",
            "80/80 [==============================] - 26s 320ms/step - loss: 0.7150 - acc: 0.7525 - val_loss: 0.6670 - val_acc: 0.7704\n",
            "Epoch 24/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.7060 - acc: 0.7574 - val_loss: 0.7433 - val_acc: 0.7475\n",
            "Epoch 25/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.7033 - acc: 0.7578 - val_loss: 0.6291 - val_acc: 0.7838\n",
            "Epoch 26/150\n",
            "80/80 [==============================] - 26s 320ms/step - loss: 0.6944 - acc: 0.7619 - val_loss: 0.6185 - val_acc: 0.7897\n",
            "Epoch 27/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.6798 - acc: 0.7642 - val_loss: 0.6292 - val_acc: 0.7869\n",
            "Epoch 28/150\n",
            "80/80 [==============================] - 26s 321ms/step - loss: 0.6719 - acc: 0.7681 - val_loss: 0.7024 - val_acc: 0.7670\n",
            "Epoch 29/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.6620 - acc: 0.7721 - val_loss: 0.6842 - val_acc: 0.7732\n",
            "Epoch 30/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.6588 - acc: 0.7731 - val_loss: 0.5970 - val_acc: 0.7983\n",
            "Epoch 31/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.6462 - acc: 0.7784 - val_loss: 0.6596 - val_acc: 0.7848\n",
            "Epoch 32/150\n",
            "80/80 [==============================] - 26s 323ms/step - loss: 0.6390 - acc: 0.7805 - val_loss: 0.6933 - val_acc: 0.7698\n",
            "Epoch 33/150\n",
            "80/80 [==============================] - 26s 320ms/step - loss: 0.6352 - acc: 0.7811 - val_loss: 0.6334 - val_acc: 0.7922\n",
            "Epoch 34/150\n",
            "80/80 [==============================] - 25s 319ms/step - loss: 0.6289 - acc: 0.7858 - val_loss: 0.5712 - val_acc: 0.8073\n",
            "Epoch 35/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.6249 - acc: 0.7834 - val_loss: 0.5834 - val_acc: 0.8025\n",
            "Epoch 36/150\n",
            "80/80 [==============================] - 26s 323ms/step - loss: 0.6213 - acc: 0.7874 - val_loss: 0.6347 - val_acc: 0.7906\n",
            "Epoch 37/150\n",
            "80/80 [==============================] - 26s 321ms/step - loss: 0.6142 - acc: 0.7894 - val_loss: 0.6047 - val_acc: 0.7978\n",
            "Epoch 38/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.6002 - acc: 0.7940 - val_loss: 0.6067 - val_acc: 0.8007\n",
            "Epoch 39/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.5982 - acc: 0.7958 - val_loss: 0.6160 - val_acc: 0.7938\n",
            "Epoch 40/150\n",
            "80/80 [==============================] - 26s 323ms/step - loss: 0.5888 - acc: 0.7997 - val_loss: 0.5675 - val_acc: 0.8122\n",
            "Epoch 41/150\n",
            "80/80 [==============================] - 26s 321ms/step - loss: 0.5899 - acc: 0.7978 - val_loss: 0.5588 - val_acc: 0.8090\n",
            "Epoch 42/150\n",
            "80/80 [==============================] - 26s 322ms/step - loss: 0.5775 - acc: 0.8029 - val_loss: 0.6329 - val_acc: 0.7959\n",
            "Epoch 43/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.5760 - acc: 0.8024 - val_loss: 0.6454 - val_acc: 0.7963\n",
            "Epoch 44/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.5669 - acc: 0.8053 - val_loss: 0.6224 - val_acc: 0.8007\n",
            "Epoch 45/150\n",
            "80/80 [==============================] - 26s 323ms/step - loss: 0.5571 - acc: 0.8087 - val_loss: 0.5578 - val_acc: 0.8167\n",
            "Epoch 46/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.5593 - acc: 0.8071 - val_loss: 0.6022 - val_acc: 0.8048\n",
            "Epoch 47/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.5508 - acc: 0.8125 - val_loss: 0.5129 - val_acc: 0.8307\n",
            "Epoch 48/150\n",
            "80/80 [==============================] - 25s 319ms/step - loss: 0.5431 - acc: 0.8145 - val_loss: 0.4989 - val_acc: 0.8284\n",
            "Epoch 49/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.5400 - acc: 0.8161 - val_loss: 0.5221 - val_acc: 0.8238\n",
            "Epoch 50/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.5361 - acc: 0.8166 - val_loss: 0.5841 - val_acc: 0.8047\n",
            "Epoch 51/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.5391 - acc: 0.8145 - val_loss: 0.5316 - val_acc: 0.8210\n",
            "Epoch 52/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.5336 - acc: 0.8182 - val_loss: 0.5285 - val_acc: 0.8285\n",
            "Epoch 53/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.5144 - acc: 0.8250 - val_loss: 0.4691 - val_acc: 0.8406\n",
            "Epoch 54/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.5255 - acc: 0.8183 - val_loss: 0.5579 - val_acc: 0.8151\n",
            "Epoch 55/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.5082 - acc: 0.8255 - val_loss: 0.5233 - val_acc: 0.8305\n",
            "Epoch 56/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.5110 - acc: 0.8241 - val_loss: 0.5426 - val_acc: 0.8243\n",
            "Epoch 57/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.5078 - acc: 0.8259 - val_loss: 0.4780 - val_acc: 0.8419\n",
            "Epoch 58/150\n",
            "80/80 [==============================] - 25s 313ms/step - loss: 0.5040 - acc: 0.8262 - val_loss: 0.4515 - val_acc: 0.8486\n",
            "Epoch 59/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.4939 - acc: 0.8307 - val_loss: 0.4979 - val_acc: 0.8356\n",
            "Epoch 60/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.4965 - acc: 0.8290 - val_loss: 0.5493 - val_acc: 0.8252\n",
            "Epoch 61/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.4906 - acc: 0.8324 - val_loss: 0.5277 - val_acc: 0.8276\n",
            "Epoch 62/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.4859 - acc: 0.8344 - val_loss: 0.5635 - val_acc: 0.8245\n",
            "Epoch 63/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.4767 - acc: 0.8359 - val_loss: 0.5663 - val_acc: 0.8215\n",
            "Epoch 64/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.4748 - acc: 0.8373 - val_loss: 0.5280 - val_acc: 0.8350\n",
            "Epoch 65/150\n",
            "80/80 [==============================] - 26s 320ms/step - loss: 0.4762 - acc: 0.8352 - val_loss: 0.4566 - val_acc: 0.8478\n",
            "Epoch 66/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.4752 - acc: 0.8368 - val_loss: 0.4818 - val_acc: 0.8403\n",
            "Epoch 67/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.4638 - acc: 0.8384 - val_loss: 0.5011 - val_acc: 0.8365\n",
            "Epoch 68/150\n",
            "80/80 [==============================] - 25s 314ms/step - loss: 0.4608 - acc: 0.8407 - val_loss: 0.4936 - val_acc: 0.8404\n",
            "Epoch 69/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.4563 - acc: 0.8439 - val_loss: 0.4820 - val_acc: 0.8441\n",
            "Epoch 70/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.4509 - acc: 0.8450 - val_loss: 0.4794 - val_acc: 0.8447\n",
            "Epoch 71/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.4484 - acc: 0.8465 - val_loss: 0.5548 - val_acc: 0.8272\n",
            "Epoch 72/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.4445 - acc: 0.8463 - val_loss: 0.4558 - val_acc: 0.8516\n",
            "Epoch 73/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.4392 - acc: 0.8503 - val_loss: 0.4728 - val_acc: 0.8471\n",
            "Epoch 74/150\n",
            "80/80 [==============================] - 25s 314ms/step - loss: 0.4396 - acc: 0.8476 - val_loss: 0.5537 - val_acc: 0.8309\n",
            "Epoch 75/150\n",
            "80/80 [==============================] - 25s 313ms/step - loss: 0.4337 - acc: 0.8504 - val_loss: 0.5024 - val_acc: 0.8422\n",
            "Epoch 76/150\n",
            "80/80 [==============================] - 26s 327ms/step - loss: 0.4355 - acc: 0.8504 - val_loss: 0.4773 - val_acc: 0.8468\n",
            "Epoch 77/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.4339 - acc: 0.8503 - val_loss: 0.4631 - val_acc: 0.8505\n",
            "Epoch 78/150\n",
            "80/80 [==============================] - 25s 315ms/step - loss: 0.4298 - acc: 0.8513 - val_loss: 0.4470 - val_acc: 0.8576\n",
            "Epoch 79/150\n",
            "80/80 [==============================] - 26s 322ms/step - loss: 0.4232 - acc: 0.8548 - val_loss: 0.4894 - val_acc: 0.8431\n",
            "Epoch 80/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.4210 - acc: 0.8566 - val_loss: 0.4796 - val_acc: 0.8440\n",
            "Epoch 81/150\n",
            "80/80 [==============================] - 25s 315ms/step - loss: 0.4093 - acc: 0.8592 - val_loss: 0.4502 - val_acc: 0.8546\n",
            "Epoch 82/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.4109 - acc: 0.8564 - val_loss: 0.4864 - val_acc: 0.8441\n",
            "Epoch 83/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.4147 - acc: 0.8571 - val_loss: 0.4705 - val_acc: 0.8494\n",
            "Epoch 84/150\n",
            "80/80 [==============================] - 25s 319ms/step - loss: 0.4134 - acc: 0.8568 - val_loss: 0.5529 - val_acc: 0.8349\n",
            "Epoch 85/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.4081 - acc: 0.8576 - val_loss: 0.4039 - val_acc: 0.8664\n",
            "Epoch 86/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3962 - acc: 0.8638 - val_loss: 0.5762 - val_acc: 0.8284\n",
            "Epoch 87/150\n",
            "80/80 [==============================] - 26s 320ms/step - loss: 0.4013 - acc: 0.8610 - val_loss: 0.4365 - val_acc: 0.8591\n",
            "Epoch 88/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3914 - acc: 0.8656 - val_loss: 0.4529 - val_acc: 0.8552\n",
            "Epoch 89/150\n",
            "80/80 [==============================] - 26s 322ms/step - loss: 0.3865 - acc: 0.8658 - val_loss: 0.4202 - val_acc: 0.8648\n",
            "Epoch 90/150\n",
            "80/80 [==============================] - 26s 320ms/step - loss: 0.3889 - acc: 0.8657 - val_loss: 0.4825 - val_acc: 0.8485\n",
            "Epoch 91/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3885 - acc: 0.8672 - val_loss: 0.5786 - val_acc: 0.8314\n",
            "Epoch 92/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3847 - acc: 0.8677 - val_loss: 0.4526 - val_acc: 0.8577\n",
            "Epoch 93/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.3851 - acc: 0.8666 - val_loss: 0.4405 - val_acc: 0.8556\n",
            "Epoch 94/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.3840 - acc: 0.8674 - val_loss: 0.4625 - val_acc: 0.8547\n",
            "Epoch 95/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3774 - acc: 0.8688 - val_loss: 0.4143 - val_acc: 0.8677\n",
            "Epoch 96/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.3735 - acc: 0.8702 - val_loss: 0.4770 - val_acc: 0.8498\n",
            "Epoch 97/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.3670 - acc: 0.8730 - val_loss: 0.4576 - val_acc: 0.8570\n",
            "Epoch 98/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.3706 - acc: 0.8738 - val_loss: 0.4439 - val_acc: 0.8577\n",
            "Epoch 99/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.3723 - acc: 0.8736 - val_loss: 0.5156 - val_acc: 0.8402\n",
            "Epoch 100/150\n",
            "80/80 [==============================] - 25s 315ms/step - loss: 0.3625 - acc: 0.8765 - val_loss: 0.4456 - val_acc: 0.8620\n",
            "Epoch 101/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3694 - acc: 0.8724 - val_loss: 0.4785 - val_acc: 0.8505\n",
            "Epoch 102/150\n",
            "80/80 [==============================] - 25s 315ms/step - loss: 0.3592 - acc: 0.8760 - val_loss: 0.3925 - val_acc: 0.8740\n",
            "Epoch 103/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3598 - acc: 0.8737 - val_loss: 0.4269 - val_acc: 0.8679\n",
            "Epoch 104/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.3541 - acc: 0.8777 - val_loss: 0.4945 - val_acc: 0.8478\n",
            "Epoch 105/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.3550 - acc: 0.8784 - val_loss: 0.5733 - val_acc: 0.8358\n",
            "Epoch 106/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3509 - acc: 0.8792 - val_loss: 0.4494 - val_acc: 0.8627\n",
            "Epoch 107/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3507 - acc: 0.8790 - val_loss: 0.3766 - val_acc: 0.8767\n",
            "Epoch 108/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3468 - acc: 0.8795 - val_loss: 0.4381 - val_acc: 0.8591\n",
            "Epoch 109/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.3443 - acc: 0.8819 - val_loss: 0.4331 - val_acc: 0.8611\n",
            "Epoch 110/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3417 - acc: 0.8828 - val_loss: 0.4003 - val_acc: 0.8728\n",
            "Epoch 111/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3421 - acc: 0.8817 - val_loss: 0.4598 - val_acc: 0.8564\n",
            "Epoch 112/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.3347 - acc: 0.8853 - val_loss: 0.4631 - val_acc: 0.8575\n",
            "Epoch 113/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.3324 - acc: 0.8850 - val_loss: 0.4065 - val_acc: 0.8691\n",
            "Epoch 114/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3301 - acc: 0.8872 - val_loss: 0.4860 - val_acc: 0.8512\n",
            "Epoch 115/150\n",
            "80/80 [==============================] - 25s 319ms/step - loss: 0.3343 - acc: 0.8854 - val_loss: 0.3943 - val_acc: 0.8716\n",
            "Epoch 116/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.3331 - acc: 0.8850 - val_loss: 0.3985 - val_acc: 0.8727\n",
            "Epoch 117/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.3249 - acc: 0.8885 - val_loss: 0.5106 - val_acc: 0.8500\n",
            "Epoch 118/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.3275 - acc: 0.8861 - val_loss: 0.4974 - val_acc: 0.8537\n",
            "Epoch 119/150\n",
            "80/80 [==============================] - 25s 315ms/step - loss: 0.3261 - acc: 0.8850 - val_loss: 0.4842 - val_acc: 0.8586\n",
            "Epoch 120/150\n",
            "80/80 [==============================] - 26s 320ms/step - loss: 0.3237 - acc: 0.8874 - val_loss: 0.4313 - val_acc: 0.8676\n",
            "Epoch 121/150\n",
            "80/80 [==============================] - 25s 315ms/step - loss: 0.3197 - acc: 0.8889 - val_loss: 0.4617 - val_acc: 0.8579\n",
            "Epoch 122/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.3224 - acc: 0.8898 - val_loss: 0.4446 - val_acc: 0.8633\n",
            "Epoch 123/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.3184 - acc: 0.8880 - val_loss: 0.4964 - val_acc: 0.8495\n",
            "Epoch 124/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3176 - acc: 0.8909 - val_loss: 0.4713 - val_acc: 0.8602\n",
            "Epoch 125/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.3074 - acc: 0.8921 - val_loss: 0.4333 - val_acc: 0.8674\n",
            "Epoch 126/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.3108 - acc: 0.8925 - val_loss: 0.4481 - val_acc: 0.8590\n",
            "Epoch 127/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3050 - acc: 0.8944 - val_loss: 0.4360 - val_acc: 0.8656\n",
            "Epoch 128/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.3071 - acc: 0.8916 - val_loss: 0.4386 - val_acc: 0.8645\n",
            "Epoch 129/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.3007 - acc: 0.8961 - val_loss: 0.4815 - val_acc: 0.8567\n",
            "Epoch 130/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.2996 - acc: 0.8961 - val_loss: 0.4288 - val_acc: 0.8683\n",
            "Epoch 131/150\n",
            "80/80 [==============================] - 25s 314ms/step - loss: 0.3024 - acc: 0.8948 - val_loss: 0.4092 - val_acc: 0.8738\n",
            "Epoch 132/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.2986 - acc: 0.8966 - val_loss: 0.4724 - val_acc: 0.8626\n",
            "Epoch 133/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.2982 - acc: 0.8970 - val_loss: 0.3915 - val_acc: 0.8749\n",
            "Epoch 134/150\n",
            "80/80 [==============================] - 25s 315ms/step - loss: 0.2981 - acc: 0.8946 - val_loss: 0.3800 - val_acc: 0.8787\n",
            "Epoch 135/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.2984 - acc: 0.8960 - val_loss: 0.4117 - val_acc: 0.8723\n",
            "Epoch 136/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.2883 - acc: 0.9006 - val_loss: 0.4170 - val_acc: 0.8754\n",
            "Epoch 137/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.2936 - acc: 0.8994 - val_loss: 0.4792 - val_acc: 0.8616\n",
            "Epoch 138/150\n",
            "80/80 [==============================] - 26s 321ms/step - loss: 0.2864 - acc: 0.8998 - val_loss: 0.4457 - val_acc: 0.8713\n",
            "Epoch 139/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.2864 - acc: 0.9015 - val_loss: 0.4358 - val_acc: 0.8712\n",
            "Epoch 140/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.2841 - acc: 0.9005 - val_loss: 0.4947 - val_acc: 0.8583\n",
            "Epoch 141/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.2845 - acc: 0.9005 - val_loss: 0.3750 - val_acc: 0.8823\n",
            "Epoch 142/150\n",
            "80/80 [==============================] - 25s 314ms/step - loss: 0.2788 - acc: 0.9043 - val_loss: 0.4112 - val_acc: 0.8751\n",
            "Epoch 143/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.2807 - acc: 0.9029 - val_loss: 0.3822 - val_acc: 0.8850\n",
            "Epoch 144/150\n",
            "80/80 [==============================] - 25s 314ms/step - loss: 0.2806 - acc: 0.9032 - val_loss: 0.4428 - val_acc: 0.8742\n",
            "Epoch 145/150\n",
            "80/80 [==============================] - 25s 317ms/step - loss: 0.2770 - acc: 0.9017 - val_loss: 0.5097 - val_acc: 0.8557\n",
            "Epoch 146/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.2716 - acc: 0.9060 - val_loss: 0.4487 - val_acc: 0.8667\n",
            "Epoch 147/150\n",
            "80/80 [==============================] - 25s 316ms/step - loss: 0.2741 - acc: 0.9050 - val_loss: 0.4017 - val_acc: 0.8808\n",
            "Epoch 148/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.2748 - acc: 0.9034 - val_loss: 0.4598 - val_acc: 0.8646\n",
            "Epoch 149/150\n",
            "80/80 [==============================] - 25s 318ms/step - loss: 0.2729 - acc: 0.9063 - val_loss: 0.3918 - val_acc: 0.8813\n",
            "Epoch 150/150\n",
            "80/80 [==============================] - 26s 319ms/step - loss: 0.2727 - acc: 0.9048 - val_loss: 0.3927 - val_acc: 0.8790\n",
            "start evaluation...\n",
            "10000/10000 [==============================] - 3s 299us/step\n",
            "Test loss: 0.4144394582390785\n",
            "Test accuracy: 0.8743\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}