{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_cnn_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oxygen0605/ImageClassification/blob/master/deep_cnn_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj4ebRdWOMpm",
        "colab_type": "text"
      },
      "source": [
        "# Google Colaboratory環境の初期設定\n",
        "\n",
        "## Google Driveにマウントしてマシンスペックを出力"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIgX8qtmOOY9",
        "colab_type": "code",
        "outputId": "a3092aa4-1b68-486b-fff2-63b8744e8a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!nvidia-smi > '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'\n",
        "!cat /proc/driver/nvidia/gpus/0000:00:04.0/information >> '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'\n",
        "!cat /etc/issue >> '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'\n",
        "!cat /proc/cpuinfo >> '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45yQPr6Lltrj",
        "colab_type": "text"
      },
      "source": [
        "# Deep CNN (CIFAR-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur3z5pdimJdX",
        "colab_type": "text"
      },
      "source": [
        "## モデルの生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CLdMi7Wlzwr",
        "colab_type": "code",
        "outputId": "9494d4e4-5920-40d7-ba2c-bd3a1a577bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Dense, BatchNormalization\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Activation, Flatten\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras import regularizers\n",
        "\n",
        "def deep_cnn(input_shape, num_classes):\n",
        "    inputs = Input(shape = input_shape)\n",
        "    \n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(inputs)\n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(512,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(512,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = Dense(1024,activation = \"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1024,activation = \"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    y  = Dense(num_classes, activation = \"softmax\")(x)\n",
        "\n",
        "    return Model(input = inputs, output = y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygmn8vNImOV-",
        "colab_type": "text"
      },
      "source": [
        "## CIFAR10 データセットの用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JELm-0dimTvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "class CIFAR10Dataset():\n",
        "\tdef __init__(self):\n",
        "\t\tself.image_shape = (32, 32, 3)\n",
        "\t\tself.num_classes = 10\n",
        "\t\t\n",
        "\tdef preprocess(self, data, label_data=False):\n",
        "\t\tif label_data:\n",
        "\t\t\t# conver class number to one-hot vector\n",
        "\t\t\tdata = keras.utils.to_categorical(data, self.num_classes)\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tdata = data.astype(\"float32\")\n",
        "\t\t\tdata /= 255 #convert the value to 0 ~ 1 scale\n",
        "\t\t\tshape = (data.shape[0],) + self.image_shape\n",
        "\t\t\tdata = data.reshape(shape)\n",
        "\t\t\t\n",
        "\t\treturn data\n",
        "\t\n",
        "\tdef get_batch(self):\n",
        "\t\t# x: data, y: lebel\n",
        "\t\t(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\t\t\n",
        "\t\tx_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
        "\t\ty_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
        "\t\t\t\t\t [y_train, y_test]]\n",
        "\t\t\n",
        "\t\treturn x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pawbuqkSPkg",
        "colab_type": "text"
      },
      "source": [
        "## TensorBoard用のログファイル生成関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEgHJML0SZdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "from time import gmtime, strftime\n",
        "from keras.callbacks import TensorBoard\n",
        "import os\n",
        "\n",
        "\n",
        "def make_tensorboard(set_dir_name=''):\n",
        "    tictoc = strftime(\"%a_%d_%b_%Y_%H_%M_%S\", gmtime())\n",
        "    directory_name = tictoc\n",
        "    log_dir = set_dir_name + '_' + directory_name\n",
        "    os.mkdir(log_dir)\n",
        "    tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, )\n",
        "    return tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TWZvmmxmsK2",
        "colab_type": "text"
      },
      "source": [
        "## Training, Evaluation用クラスの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxj9ZLVbmqnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
        "from keras.optimizers import RMSprop, Adam, Nadam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "from time import gmtime, strftime\n",
        "import os\n",
        "\n",
        "\n",
        "def make_tensorboard(set_dir_name=''):\n",
        "    tictoc = strftime(\"%a_%d_%b_%Y_%H_%M_%S\", gmtime())\n",
        "    directory_name = tictoc\n",
        "    log_dir = set_dir_name + '_' + directory_name\n",
        "    os.mkdir(log_dir)\n",
        "    tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, )\n",
        "    return tensorboard\n",
        "\n",
        "class Trainer():\n",
        "\t\n",
        "\tdef __init__(self, model, loss, optimizer, logdir = './'):\n",
        "\t\tself._target = model\n",
        "\t\tself._target.compile(\n",
        "\t\t\t\tloss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
        "\t\t\t\t)\n",
        "\t\tself.verbose = 1 # visualize progress bar: 0(OFF), 1(On), 2(On:each data) \n",
        "\t\t#self.log_dir = os.path.join(os.path.dirname(__file__), logdir)\n",
        "\t\tself.log_dir = os.path.join(logdir)\n",
        "\t\tself.model_file_name = \"model_file.hdf5\"\n",
        "\t\n",
        "\tdef train_for_tuning_test_data(self, \n",
        "            x_train, y_train, x_test, y_test, batch_size, epochs, lr_scheduler):\n",
        "\t\tdatagen = ImageDataGenerator(\n",
        "\t\t\tfeaturewise_center=False,            # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,             # set each sample mean to 0\n",
        "            featurewise_std_normalization=False, # divide inputs by std\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,                 # apply ZCA whitening\n",
        "            rotation_range=20,                   # randomly rotate images in the range (0~180)\n",
        "            width_shift_range=0.2,               # randomly shift images horizontally\n",
        "            height_shift_range=0.2,              # randomly shift images vertically\n",
        "            zoom_range = 0.2,\n",
        "            channel_shift_range = 0.2,\n",
        "            horizontal_flip=True,                # randomly flip images\n",
        "            vertical_flip=False                  # randomly flip images\n",
        "\t\t)\n",
        "        \n",
        "    # training (validation dataはデータ拡張はしない)\n",
        "\t\tmodel_path = os.path.join(self.log_dir, self.model_file_name)\n",
        "\t\tself._target.fit_generator(\n",
        "            generator        = datagen.flow(x_train,y_train, batch_size),\n",
        "            steps_per_epoch  = x_train.shape[0] // batch_size,\n",
        "            epochs           = epochs,\n",
        "            validation_data  = ImageDataGenerator().flow(x_test,y_test, batch_size),\n",
        "\t\t\t      validation_steps = x_test.shape[0] // batch_size,\n",
        "            callbacks=[\n",
        "                LearningRateScheduler(lr_scheduler),\n",
        "                make_tensorboard(set_dir_name=self.log_dir),\n",
        "                ModelCheckpoint(model_path, save_best_only=True)\n",
        "            ],\n",
        "            verbose = self.verbose,\n",
        "            workers = 4\n",
        "        )\n",
        "\t\t\n",
        "\n",
        "class Evaluator():\n",
        "    \n",
        "    def __init__(self, result_file_path=\"./prediction_result.csv\"):\n",
        "        self.result_file_path=\"./prediction_result.csv\"\n",
        "        \n",
        "    def simple_evaluate(self, model, x_test, label):\n",
        "        print(\"start evaluation...\")\n",
        "        score = model.evaluate(x_test, y_test, verbose=1)\n",
        "        print(\"Test loss:\", score[0])\n",
        "        print(\"Test accuracy:\", score[1])\n",
        "        return score\n",
        "    \n",
        "    def tta_evaluate(self, model, x_test, label, batch_size = 2500, tta_epochs = 2):\n",
        "        print(\"batch size (TTA): \"+str(batch_size))\n",
        "        print(\"epochs (TTA): \"+str(tta_epochs))\n",
        "        tta = TTA()\n",
        "        tta_pred = tta.predict(model, x_test, batch_size, epochs = tta_epochs)\n",
        "        print(\"Test accuracy(TTA): \",end = \"\")\n",
        "        print( accuracy_score( np.argmax(tta_pred,axis = 1) , np.argmax(label,axis = 1)))\n",
        "        return tta_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCiJEpaB-HAv",
        "colab_type": "text"
      },
      "source": [
        "## 学習率減衰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4me_PZO-F2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_rate_schedule_for_Adam(epoch):\n",
        "\tlr = 0.001\n",
        "\tif(epoch >= 100): lr = 0.0002 \n",
        "\tif(epoch >= 140): lr = 0.0001\n",
        "\treturn lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKULnAcX2ilW",
        "colab_type": "text"
      },
      "source": [
        "## 実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzvig0oHnqoY",
        "colab_type": "code",
        "outputId": "9c70c73c-e393-4845-c8c1-ee37809849a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "# create dataset\n",
        "dataset = CIFAR10Dataset()\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "\n",
        "# create model\n",
        "model = deep_cnn(dataset.image_shape, dataset.num_classes)\n",
        "\n",
        "# train the model\n",
        "# RMSprpの方がいいかもしれない\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=Adam(), logdir='/content/drive/My Drive/Colab Notebooks/Logs/deep_cnn_2')\n",
        "trainer.train_for_tuning_test_data(\n",
        "            x_train, y_train, x_test, y_test, batch_size=500, epochs=150, \n",
        "            lr_scheduler=learning_rate_schedule_for_Adam)\n",
        "\t\n",
        "\n",
        "# show result\n",
        "evaluator = Evaluator()\n",
        "score = evaluator.simple_evaluate(model, x_test, y_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 13s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0818 05:50:44.914837 139961580173184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0818 05:50:44.951925 139961580173184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0818 05:50:44.962502 139961580173184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0818 05:50:45.018525 139961580173184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0818 05:50:45.019335 139961580173184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0818 05:50:48.088142 139961580173184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0818 05:50:48.166533 139961580173184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0818 05:50:48.174445 139961580173184 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "W0818 05:50:48.584850 139961580173184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0818 05:50:50.995926 139961580173184 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0818 05:50:53.207422 139961580173184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0818 05:50:53.208691 139961580173184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "100/100 [==============================] - 43s 430ms/step - loss: 1.9527 - acc: 0.2352 - val_loss: 2.3619 - val_acc: 0.2866\n",
            "Epoch 2/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 1.6230 - acc: 0.3847 - val_loss: 3.4643 - val_acc: 0.2909\n",
            "Epoch 3/150\n",
            "100/100 [==============================] - 33s 329ms/step - loss: 1.4136 - acc: 0.4789 - val_loss: 3.1220 - val_acc: 0.2869\n",
            "Epoch 4/150\n",
            "100/100 [==============================] - 33s 329ms/step - loss: 1.2674 - acc: 0.5440 - val_loss: 2.6944 - val_acc: 0.3800\n",
            "Epoch 5/150\n",
            "100/100 [==============================] - 33s 329ms/step - loss: 1.1484 - acc: 0.5946 - val_loss: 1.4794 - val_acc: 0.5477\n",
            "Epoch 6/150\n",
            "100/100 [==============================] - 33s 331ms/step - loss: 1.0474 - acc: 0.6317 - val_loss: 1.8338 - val_acc: 0.5064\n",
            "Epoch 7/150\n",
            "100/100 [==============================] - 33s 332ms/step - loss: 0.9897 - acc: 0.6556 - val_loss: 1.1791 - val_acc: 0.6309\n",
            "Epoch 8/150\n",
            "100/100 [==============================] - 33s 331ms/step - loss: 0.9117 - acc: 0.6867 - val_loss: 1.2059 - val_acc: 0.6263\n",
            "Epoch 9/150\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 0.8592 - acc: 0.7054 - val_loss: 1.0927 - val_acc: 0.6631\n",
            "Epoch 10/150\n",
            "100/100 [==============================] - 33s 332ms/step - loss: 0.8184 - acc: 0.7208 - val_loss: 1.0475 - val_acc: 0.6878\n",
            "Epoch 11/150\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 0.7714 - acc: 0.7394 - val_loss: 0.8037 - val_acc: 0.7360\n",
            "Epoch 12/150\n",
            "100/100 [==============================] - 33s 327ms/step - loss: 0.7420 - acc: 0.7505 - val_loss: 0.9506 - val_acc: 0.7261\n",
            "Epoch 13/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.7089 - acc: 0.7604 - val_loss: 1.0466 - val_acc: 0.6859\n",
            "Epoch 14/150\n",
            "100/100 [==============================] - 32s 325ms/step - loss: 0.6836 - acc: 0.7695 - val_loss: 0.6707 - val_acc: 0.7799\n",
            "Epoch 15/150\n",
            "100/100 [==============================] - 33s 325ms/step - loss: 0.6610 - acc: 0.7783 - val_loss: 0.6833 - val_acc: 0.7880\n",
            "Epoch 16/150\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 0.6323 - acc: 0.7884 - val_loss: 0.6758 - val_acc: 0.7824\n",
            "Epoch 17/150\n",
            "100/100 [==============================] - 33s 325ms/step - loss: 0.6133 - acc: 0.7936 - val_loss: 0.6027 - val_acc: 0.8077\n",
            "Epoch 18/150\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 0.5940 - acc: 0.8015 - val_loss: 0.7408 - val_acc: 0.7784\n",
            "Epoch 19/150\n",
            "100/100 [==============================] - 33s 326ms/step - loss: 0.5772 - acc: 0.8061 - val_loss: 0.6586 - val_acc: 0.7918\n",
            "Epoch 20/150\n",
            "100/100 [==============================] - 33s 329ms/step - loss: 0.5525 - acc: 0.8148 - val_loss: 0.7617 - val_acc: 0.7818\n",
            "Epoch 21/150\n",
            "100/100 [==============================] - 33s 325ms/step - loss: 0.5508 - acc: 0.8148 - val_loss: 0.5444 - val_acc: 0.8269\n",
            "Epoch 22/150\n",
            "100/100 [==============================] - 33s 327ms/step - loss: 0.5337 - acc: 0.8205 - val_loss: 0.5358 - val_acc: 0.8298\n",
            "Epoch 23/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.5190 - acc: 0.8276 - val_loss: 0.6776 - val_acc: 0.8034\n",
            "Epoch 24/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.5090 - acc: 0.8279 - val_loss: 0.5834 - val_acc: 0.8179\n",
            "Epoch 25/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.4976 - acc: 0.8334 - val_loss: 0.4943 - val_acc: 0.8387\n",
            "Epoch 26/150\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 0.4862 - acc: 0.8363 - val_loss: 0.6335 - val_acc: 0.8059\n",
            "Epoch 27/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.4734 - acc: 0.8410 - val_loss: 0.4367 - val_acc: 0.8587\n",
            "Epoch 28/150\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 0.4717 - acc: 0.8424 - val_loss: 0.5610 - val_acc: 0.8271\n",
            "Epoch 29/150\n",
            "100/100 [==============================] - 33s 325ms/step - loss: 0.4584 - acc: 0.8467 - val_loss: 0.5039 - val_acc: 0.8387\n",
            "Epoch 30/150\n",
            "100/100 [==============================] - 33s 325ms/step - loss: 0.4464 - acc: 0.8490 - val_loss: 0.5930 - val_acc: 0.8267\n",
            "Epoch 31/150\n",
            "100/100 [==============================] - 32s 325ms/step - loss: 0.4434 - acc: 0.8518 - val_loss: 0.4535 - val_acc: 0.8568\n",
            "Epoch 32/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.4327 - acc: 0.8537 - val_loss: 0.5450 - val_acc: 0.8365\n",
            "Epoch 33/150\n",
            "100/100 [==============================] - 33s 327ms/step - loss: 0.4207 - acc: 0.8591 - val_loss: 0.5210 - val_acc: 0.8408\n",
            "Epoch 34/150\n",
            "100/100 [==============================] - 32s 325ms/step - loss: 0.4210 - acc: 0.8589 - val_loss: 0.4521 - val_acc: 0.8568\n",
            "Epoch 35/150\n",
            "100/100 [==============================] - 33s 326ms/step - loss: 0.4130 - acc: 0.8614 - val_loss: 0.5525 - val_acc: 0.8369\n",
            "Epoch 36/150\n",
            "100/100 [==============================] - 33s 325ms/step - loss: 0.4074 - acc: 0.8652 - val_loss: 0.4169 - val_acc: 0.8640\n",
            "Epoch 37/150\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 0.3978 - acc: 0.8674 - val_loss: 0.3630 - val_acc: 0.8801\n",
            "Epoch 38/150\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 0.3965 - acc: 0.8671 - val_loss: 0.4496 - val_acc: 0.8605\n",
            "Epoch 39/150\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 0.3913 - acc: 0.8686 - val_loss: 0.4761 - val_acc: 0.8574\n",
            "Epoch 40/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.3792 - acc: 0.8725 - val_loss: 0.3827 - val_acc: 0.8735\n",
            "Epoch 41/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.3771 - acc: 0.8734 - val_loss: 0.4476 - val_acc: 0.8672\n",
            "Epoch 42/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.3717 - acc: 0.8767 - val_loss: 0.5023 - val_acc: 0.8457\n",
            "Epoch 43/150\n",
            "100/100 [==============================] - 32s 325ms/step - loss: 0.3683 - acc: 0.8754 - val_loss: 0.4349 - val_acc: 0.8664\n",
            "Epoch 44/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.3656 - acc: 0.8777 - val_loss: 0.3863 - val_acc: 0.8778\n",
            "Epoch 45/150\n",
            "100/100 [==============================] - 33s 327ms/step - loss: 0.3566 - acc: 0.8803 - val_loss: 0.4064 - val_acc: 0.8774\n",
            "Epoch 46/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.3558 - acc: 0.8804 - val_loss: 0.4794 - val_acc: 0.8539\n",
            "Epoch 47/150\n",
            "100/100 [==============================] - 33s 326ms/step - loss: 0.3432 - acc: 0.8844 - val_loss: 0.3914 - val_acc: 0.8807\n",
            "Epoch 48/150\n",
            "100/100 [==============================] - 33s 329ms/step - loss: 0.3451 - acc: 0.8842 - val_loss: 0.4896 - val_acc: 0.8592\n",
            "Epoch 49/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.3404 - acc: 0.8864 - val_loss: 0.4041 - val_acc: 0.8723\n",
            "Epoch 50/150\n",
            "100/100 [==============================] - 33s 326ms/step - loss: 0.3374 - acc: 0.8863 - val_loss: 0.3789 - val_acc: 0.8813\n",
            "Epoch 51/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.3284 - acc: 0.8900 - val_loss: 0.4207 - val_acc: 0.8729\n",
            "Epoch 52/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.3291 - acc: 0.8893 - val_loss: 0.3740 - val_acc: 0.8815\n",
            "Epoch 53/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.3312 - acc: 0.8903 - val_loss: 0.3668 - val_acc: 0.8846\n",
            "Epoch 54/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.3215 - acc: 0.8903 - val_loss: 0.4628 - val_acc: 0.8660\n",
            "Epoch 55/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.3167 - acc: 0.8933 - val_loss: 0.3898 - val_acc: 0.8800\n",
            "Epoch 56/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.3191 - acc: 0.8927 - val_loss: 0.3502 - val_acc: 0.8923\n",
            "Epoch 57/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.3180 - acc: 0.8943 - val_loss: 0.3446 - val_acc: 0.8914\n",
            "Epoch 58/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.3096 - acc: 0.8968 - val_loss: 0.4114 - val_acc: 0.8752\n",
            "Epoch 59/150\n",
            "100/100 [==============================] - 33s 329ms/step - loss: 0.3004 - acc: 0.8995 - val_loss: 0.4238 - val_acc: 0.8734\n",
            "Epoch 60/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.3016 - acc: 0.8970 - val_loss: 0.3366 - val_acc: 0.8979\n",
            "Epoch 61/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.2921 - acc: 0.9020 - val_loss: 0.3272 - val_acc: 0.8965\n",
            "Epoch 62/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.2985 - acc: 0.8987 - val_loss: 0.4842 - val_acc: 0.8642\n",
            "Epoch 63/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.2918 - acc: 0.9003 - val_loss: 0.4598 - val_acc: 0.8659\n",
            "Epoch 64/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.2920 - acc: 0.9013 - val_loss: 0.3597 - val_acc: 0.8907\n",
            "Epoch 65/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.2839 - acc: 0.9041 - val_loss: 0.3947 - val_acc: 0.8835\n",
            "Epoch 66/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.2868 - acc: 0.9054 - val_loss: 0.3167 - val_acc: 0.8990\n",
            "Epoch 67/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.2834 - acc: 0.9040 - val_loss: 0.4477 - val_acc: 0.8722\n",
            "Epoch 68/150\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.2795 - acc: 0.9059 - val_loss: 0.3978 - val_acc: 0.8801\n",
            "Epoch 69/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.2712 - acc: 0.9089 - val_loss: 0.3754 - val_acc: 0.8899\n",
            "Epoch 70/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.2785 - acc: 0.9059 - val_loss: 0.4532 - val_acc: 0.8752\n",
            "Epoch 71/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.2758 - acc: 0.9064 - val_loss: 0.3708 - val_acc: 0.8868\n",
            "Epoch 72/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.2745 - acc: 0.9064 - val_loss: 0.4302 - val_acc: 0.8756\n",
            "Epoch 73/150\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.2677 - acc: 0.9103 - val_loss: 0.3303 - val_acc: 0.9022\n",
            "Epoch 74/150\n",
            "100/100 [==============================] - 32s 325ms/step - loss: 0.2644 - acc: 0.9113 - val_loss: 0.3100 - val_acc: 0.9044\n",
            "Epoch 75/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.2665 - acc: 0.9106 - val_loss: 0.4298 - val_acc: 0.8811\n",
            "Epoch 76/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.2594 - acc: 0.9133 - val_loss: 0.3170 - val_acc: 0.9051\n",
            "Epoch 77/150\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.2561 - acc: 0.9141 - val_loss: 0.3628 - val_acc: 0.8922\n",
            "Epoch 78/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.2584 - acc: 0.9122 - val_loss: 0.4504 - val_acc: 0.8677\n",
            "Epoch 79/150\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.2545 - acc: 0.9143 - val_loss: 0.3499 - val_acc: 0.8878\n",
            "Epoch 80/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.2483 - acc: 0.9170 - val_loss: 0.4020 - val_acc: 0.8864\n",
            "Epoch 81/150\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.2520 - acc: 0.9154 - val_loss: 0.3893 - val_acc: 0.8846\n",
            "Epoch 82/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.2481 - acc: 0.9165 - val_loss: 0.3154 - val_acc: 0.9057\n",
            "Epoch 83/150\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.2460 - acc: 0.9167 - val_loss: 0.3841 - val_acc: 0.8951\n",
            "Epoch 84/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.2421 - acc: 0.9177 - val_loss: 0.4147 - val_acc: 0.8872\n",
            "Epoch 85/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.2429 - acc: 0.9173 - val_loss: 0.3106 - val_acc: 0.9065\n",
            "Epoch 86/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.2404 - acc: 0.9183 - val_loss: 0.3553 - val_acc: 0.8906\n",
            "Epoch 87/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.2378 - acc: 0.9180 - val_loss: 0.3726 - val_acc: 0.8943\n",
            "Epoch 88/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.2416 - acc: 0.9195 - val_loss: 0.3124 - val_acc: 0.9087\n",
            "Epoch 89/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.2364 - acc: 0.9202 - val_loss: 0.4288 - val_acc: 0.8854\n",
            "Epoch 90/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.2317 - acc: 0.9211 - val_loss: 0.3770 - val_acc: 0.8916\n",
            "Epoch 91/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.2328 - acc: 0.9220 - val_loss: 0.3944 - val_acc: 0.8919\n",
            "Epoch 92/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.2301 - acc: 0.9219 - val_loss: 0.3302 - val_acc: 0.9034\n",
            "Epoch 93/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.2306 - acc: 0.9230 - val_loss: 0.3953 - val_acc: 0.8845\n",
            "Epoch 94/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.2262 - acc: 0.9230 - val_loss: 0.4487 - val_acc: 0.8744\n",
            "Epoch 95/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.2285 - acc: 0.9221 - val_loss: 0.3054 - val_acc: 0.9083\n",
            "Epoch 96/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.2261 - acc: 0.9241 - val_loss: 0.4220 - val_acc: 0.8831\n",
            "Epoch 97/150\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.2173 - acc: 0.9271 - val_loss: 0.3770 - val_acc: 0.8979\n",
            "Epoch 98/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.2238 - acc: 0.9249 - val_loss: 0.3507 - val_acc: 0.8957\n",
            "Epoch 99/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.2160 - acc: 0.9279 - val_loss: 0.3960 - val_acc: 0.8917\n",
            "Epoch 100/150\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.2205 - acc: 0.9247 - val_loss: 0.3308 - val_acc: 0.9049\n",
            "Epoch 101/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.1801 - acc: 0.9384 - val_loss: 0.2825 - val_acc: 0.9177\n",
            "Epoch 102/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.1693 - acc: 0.9422 - val_loss: 0.2710 - val_acc: 0.9207\n",
            "Epoch 103/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.1652 - acc: 0.9444 - val_loss: 0.2657 - val_acc: 0.9242\n",
            "Epoch 104/150\n",
            "100/100 [==============================] - 33s 325ms/step - loss: 0.1554 - acc: 0.9463 - val_loss: 0.2972 - val_acc: 0.9176\n",
            "Epoch 105/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.1568 - acc: 0.9467 - val_loss: 0.2985 - val_acc: 0.9161\n",
            "Epoch 106/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.1541 - acc: 0.9476 - val_loss: 0.2836 - val_acc: 0.9179\n",
            "Epoch 107/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.1516 - acc: 0.9471 - val_loss: 0.2753 - val_acc: 0.9216\n",
            "Epoch 108/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.1472 - acc: 0.9494 - val_loss: 0.2795 - val_acc: 0.9200\n",
            "Epoch 109/150\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.1461 - acc: 0.9492 - val_loss: 0.2598 - val_acc: 0.9257\n",
            "Epoch 110/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.1438 - acc: 0.9503 - val_loss: 0.2816 - val_acc: 0.9214\n",
            "Epoch 111/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.1434 - acc: 0.9519 - val_loss: 0.2853 - val_acc: 0.9191\n",
            "Epoch 112/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.1404 - acc: 0.9522 - val_loss: 0.2892 - val_acc: 0.9165\n",
            "Epoch 113/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.1411 - acc: 0.9514 - val_loss: 0.2648 - val_acc: 0.9211\n",
            "Epoch 114/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.1388 - acc: 0.9522 - val_loss: 0.2887 - val_acc: 0.9184\n",
            "Epoch 115/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.1349 - acc: 0.9547 - val_loss: 0.2912 - val_acc: 0.9173\n",
            "Epoch 116/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.1381 - acc: 0.9531 - val_loss: 0.2647 - val_acc: 0.9251\n",
            "Epoch 117/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.1351 - acc: 0.9534 - val_loss: 0.2814 - val_acc: 0.9223\n",
            "Epoch 118/150\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.1362 - acc: 0.9527 - val_loss: 0.2657 - val_acc: 0.9246\n",
            "Epoch 119/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.1348 - acc: 0.9533 - val_loss: 0.2821 - val_acc: 0.9210\n",
            "Epoch 120/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.1345 - acc: 0.9534 - val_loss: 0.2871 - val_acc: 0.9211\n",
            "Epoch 121/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.1305 - acc: 0.9555 - val_loss: 0.2738 - val_acc: 0.9234\n",
            "Epoch 122/150\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.1295 - acc: 0.9554 - val_loss: 0.2915 - val_acc: 0.9192\n",
            "Epoch 123/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.1339 - acc: 0.9536 - val_loss: 0.2792 - val_acc: 0.9225\n",
            "Epoch 124/150\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.1324 - acc: 0.9553 - val_loss: 0.3005 - val_acc: 0.9185\n",
            "Epoch 125/150\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.1268 - acc: 0.9565 - val_loss: 0.2745 - val_acc: 0.9265\n",
            "Epoch 126/150\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.1274 - acc: 0.9568 - val_loss: 0.2840 - val_acc: 0.9217\n",
            "Epoch 127/150\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.1263 - acc: 0.9562 - val_loss: 0.2768 - val_acc: 0.9228\n",
            "Epoch 128/150\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.1223 - acc: 0.9587 - val_loss: 0.2893 - val_acc: 0.9210\n",
            "Epoch 129/150\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.1220 - acc: 0.9581 - val_loss: 0.2802 - val_acc: 0.9228\n",
            "Epoch 130/150\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.1256 - acc: 0.9571 - val_loss: 0.2937 - val_acc: 0.9184\n",
            "Epoch 131/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.1235 - acc: 0.9581 - val_loss: 0.2914 - val_acc: 0.9204\n",
            "Epoch 132/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.1250 - acc: 0.9577 - val_loss: 0.2708 - val_acc: 0.9257\n",
            "Epoch 133/150\n",
            "100/100 [==============================] - 31s 315ms/step - loss: 0.1232 - acc: 0.9578 - val_loss: 0.2916 - val_acc: 0.9210\n",
            "Epoch 134/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.1265 - acc: 0.9563 - val_loss: 0.2745 - val_acc: 0.9249\n",
            "Epoch 135/150\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.1223 - acc: 0.9578 - val_loss: 0.2922 - val_acc: 0.9208\n",
            "Epoch 136/150\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.1245 - acc: 0.9581 - val_loss: 0.2861 - val_acc: 0.9209\n",
            "Epoch 137/150\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.1203 - acc: 0.9587 - val_loss: 0.2786 - val_acc: 0.9242\n",
            "Epoch 138/150\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.1196 - acc: 0.9585 - val_loss: 0.2905 - val_acc: 0.9233\n",
            "Epoch 139/150\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.1221 - acc: 0.9575 - val_loss: 0.3032 - val_acc: 0.9190\n",
            "Epoch 140/150\n",
            "100/100 [==============================] - 31s 315ms/step - loss: 0.1169 - acc: 0.9602 - val_loss: 0.2870 - val_acc: 0.9238\n",
            "Epoch 141/150\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.1176 - acc: 0.9588 - val_loss: 0.2805 - val_acc: 0.9263\n",
            "Epoch 142/150\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.1125 - acc: 0.9602 - val_loss: 0.3036 - val_acc: 0.9209\n",
            "Epoch 143/150\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.1141 - acc: 0.9612 - val_loss: 0.2827 - val_acc: 0.9235\n",
            "Epoch 144/150\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.1115 - acc: 0.9625 - val_loss: 0.2856 - val_acc: 0.9250\n",
            "Epoch 145/150\n",
            "100/100 [==============================] - 32s 315ms/step - loss: 0.1119 - acc: 0.9610 - val_loss: 0.2822 - val_acc: 0.9251\n",
            "Epoch 146/150\n",
            "100/100 [==============================] - 31s 315ms/step - loss: 0.1062 - acc: 0.9629 - val_loss: 0.2926 - val_acc: 0.9249\n",
            "Epoch 147/150\n",
            "100/100 [==============================] - 32s 315ms/step - loss: 0.1098 - acc: 0.9621 - val_loss: 0.2909 - val_acc: 0.9244\n",
            "Epoch 148/150\n",
            "100/100 [==============================] - 33s 331ms/step - loss: 0.1083 - acc: 0.9615 - val_loss: 0.2932 - val_acc: 0.9228\n",
            "Epoch 149/150\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.1102 - acc: 0.9634 - val_loss: 0.2859 - val_acc: 0.9259\n",
            "Epoch 150/150\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.1098 - acc: 0.9626 - val_loss: 0.2899 - val_acc: 0.9229\n",
            "start evaluation...\n",
            "10000/10000 [==============================] - 3s 299us/step\n",
            "Test loss: 0.2898834700062871\n",
            "Test accuracy: 0.9229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mywGzeIugMUf",
        "colab_type": "text"
      },
      "source": [
        "##  Test Time Augmentation（TTA）を用いた推論 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkHjWCv1gLi6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "outputId": "4514d1b1-6cdd-4c85-ea81-65153bbb0e65"
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "class TTA:\n",
        "    \n",
        "    #test_time_augmentation\n",
        "    #batch_sizeは，test_sizeの約数!!!\n",
        "    def predict(self, model, x_test, batch_size ,epochs = 10):\n",
        "        \n",
        "        # Augmentation用generatorによるデータセットの作成\n",
        "        data_flow = self.generator(x_test, batch_size)\n",
        "        \n",
        "        test_size = x_test.shape[0]\n",
        "        pred = np.zeros(shape = (test_size,10), dtype = float)\n",
        "        \n",
        "        step_per_epoch = test_size //batch_size\n",
        "        for epoch in range(epochs):\n",
        "            print( 'epoch: ' + str(epoch+1)+'/'+str(epochs))\n",
        "            for step in range(step_per_epoch):\n",
        "                #print( 'step: ' + str(step+1)+'/'+str(step_per_epoch))\n",
        "                sta = batch_size * step\n",
        "                end = sta + batch_size\n",
        "                tmp_x = data_flow.__next__()\n",
        "                pred[sta:end] += model.predict(tmp_x)        \n",
        "        return pred / epochs\n",
        "    \n",
        "    \n",
        "    def generator(self, x_test,batch_size):\n",
        "        return ImageDataGenerator(\n",
        "                    rotation_range = 20,\n",
        "                    horizontal_flip = True,\n",
        "                    height_shift_range = 0.2,\n",
        "                    width_shift_range = 0.2,\n",
        "                    zoom_range = 0.2,\n",
        "                    channel_shift_range = 0.2\n",
        "                ).flow(x_test,batch_size = batch_size,shuffle = False)\n",
        "\n",
        "      \n",
        "# show result\n",
        "evaluator = Evaluator()\n",
        "score = evaluator.tta_evaluate(model, x_test, y_test, batch_size = 500, tta_epochs = 50)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch size (TTA): 500\n",
            "epochs (TTA): 50\n",
            "epoch: 1/50\n",
            "epoch: 2/50\n",
            "epoch: 3/50\n",
            "epoch: 4/50\n",
            "epoch: 5/50\n",
            "epoch: 6/50\n",
            "epoch: 7/50\n",
            "epoch: 8/50\n",
            "epoch: 9/50\n",
            "epoch: 10/50\n",
            "epoch: 11/50\n",
            "epoch: 12/50\n",
            "epoch: 13/50\n",
            "epoch: 14/50\n",
            "epoch: 15/50\n",
            "epoch: 16/50\n",
            "epoch: 17/50\n",
            "epoch: 18/50\n",
            "epoch: 19/50\n",
            "epoch: 20/50\n",
            "epoch: 21/50\n",
            "epoch: 22/50\n",
            "epoch: 23/50\n",
            "epoch: 24/50\n",
            "epoch: 25/50\n",
            "epoch: 26/50\n",
            "epoch: 27/50\n",
            "epoch: 28/50\n",
            "epoch: 29/50\n",
            "epoch: 30/50\n",
            "epoch: 31/50\n",
            "epoch: 32/50\n",
            "epoch: 33/50\n",
            "epoch: 34/50\n",
            "epoch: 35/50\n",
            "epoch: 36/50\n",
            "epoch: 37/50\n",
            "epoch: 38/50\n",
            "epoch: 39/50\n",
            "epoch: 40/50\n",
            "epoch: 41/50\n",
            "epoch: 42/50\n",
            "epoch: 43/50\n",
            "epoch: 44/50\n",
            "epoch: 45/50\n",
            "epoch: 46/50\n",
            "epoch: 47/50\n",
            "epoch: 48/50\n",
            "epoch: 49/50\n",
            "epoch: 50/50\n",
            "Test accuracy(TTA): 0.9412\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}