{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_cnn_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oxygen0605/ImageClassification/blob/master/deep_cnn_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj4ebRdWOMpm",
        "colab_type": "text"
      },
      "source": [
        "# Google Colaboratory環境の初期設定\n",
        "\n",
        "## Google Driveにマウントしてマシンスペックを出力"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIgX8qtmOOY9",
        "colab_type": "code",
        "outputId": "5ae9cf60-595e-4d03-a30c-b7dbcfed9697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!nvidia-smi > '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'\n",
        "!cat /proc/driver/nvidia/gpus/0000:00:04.0/information >> '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'\n",
        "!cat /etc/issue >> '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'\n",
        "!cat /proc/cpuinfo >> '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5whB14f2LCXO",
        "colab_type": "code",
        "outputId": "534737b1-e155-4e4e-f21d-6ef47a4eef11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "!ls -al"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Aug 18 14:45 .\n",
            "drwxr-xr-x 1 root root 4096 Aug 18 14:42 ..\n",
            "drwxr-xr-x 1 root root 4096 Aug 13 16:04 .config\n",
            "drwx------ 3 root root 4096 Aug 18 14:45 drive\n",
            "drwxr-xr-x 1 root root 4096 Aug  2 16:06 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45yQPr6Lltrj",
        "colab_type": "text"
      },
      "source": [
        "# Deep CNN (CIFAR-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur3z5pdimJdX",
        "colab_type": "text"
      },
      "source": [
        "## モデルの生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CLdMi7Wlzwr",
        "colab_type": "code",
        "outputId": "c9afc87e-ced8-4a71-a5fa-cd0f5511f829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Dense, BatchNormalization\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Activation, Flatten\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras import regularizers\n",
        "\n",
        "def deep_cnn(input_shape, num_classes):\n",
        "    inputs = Input(shape = input_shape)\n",
        "    \n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(inputs)\n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(512,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(512,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = Dense(1024,activation = \"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1024,activation = \"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    y  = Dense(num_classes, activation = \"softmax\")(x)\n",
        "\n",
        "    return Model(input = inputs, output = y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygmn8vNImOV-",
        "colab_type": "text"
      },
      "source": [
        "## CIFAR10 データセットの用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JELm-0dimTvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "class CIFAR10Dataset():\n",
        "\tdef __init__(self):\n",
        "\t\tself.image_shape = (32, 32, 3)\n",
        "\t\tself.num_classes = 10\n",
        "\t\t\n",
        "\tdef preprocess(self, data, label_data=False):\n",
        "\t\tif label_data:\n",
        "\t\t\t# conver class number to one-hot vector\n",
        "\t\t\tdata = keras.utils.to_categorical(data, self.num_classes)\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tdata = data.astype(\"float32\")\n",
        "\t\t\tdata /= 255 #convert the value to 0 ~ 1 scale\n",
        "\t\t\tshape = (data.shape[0],) + self.image_shape\n",
        "\t\t\tdata = data.reshape(shape)\n",
        "\t\t\t\n",
        "\t\treturn data\n",
        "\t\n",
        "\tdef get_batch(self):\n",
        "\t\t# x: data, y: lebel\n",
        "\t\t(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\t\t\n",
        "\t\tx_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
        "\t\ty_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
        "\t\t\t\t\t [y_train, y_test]]\n",
        "\t\t\n",
        "\t\treturn x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pawbuqkSPkg",
        "colab_type": "text"
      },
      "source": [
        "## TensorBoard用のログファイル生成関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEgHJML0SZdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "from time import gmtime, strftime\n",
        "from keras.callbacks import TensorBoard\n",
        "import os\n",
        "\n",
        "\n",
        "def make_tensorboard(set_dir_name=''):\n",
        "    tictoc = strftime(\"%a_%d_%b_%Y_%H_%M_%S\", gmtime())\n",
        "    directory_name = tictoc\n",
        "    log_dir = set_dir_name + '_' + directory_name\n",
        "    os.mkdir(log_dir)\n",
        "    tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, )\n",
        "    return tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TWZvmmxmsK2",
        "colab_type": "text"
      },
      "source": [
        "## Training, Evaluation用クラスの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxj9ZLVbmqnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
        "from keras.optimizers import RMSprop, Adam, Nadam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "from time import gmtime, strftime\n",
        "import os\n",
        "\n",
        "\n",
        "def make_tensorboard(set_dir_name=''):\n",
        "    tictoc = strftime(\"%a_%d_%b_%Y_%H_%M_%S\", gmtime())\n",
        "    directory_name = tictoc\n",
        "    log_dir = set_dir_name + '_' + directory_name\n",
        "    os.mkdir(log_dir)\n",
        "    tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, )\n",
        "    return tensorboard\n",
        "\n",
        "class Trainer():\n",
        "\t\n",
        "\tdef __init__(self, model, loss, optimizer, logdir = './'):\n",
        "\t\tself._target = model\n",
        "\t\tself._target.compile(\n",
        "\t\t\t\tloss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
        "\t\t\t\t)\n",
        "\t\tself.verbose = 1 # visualize progress bar: 0(OFF), 1(On), 2(On:each data) \n",
        "\t\t#self.log_dir = os.path.join(os.path.dirname(__file__), logdir)\n",
        "\t\tself.log_dir = os.path.join(logdir)\n",
        "\t\tself.model_file_name = \"model_file.hdf5\"\n",
        "\t\n",
        "\tdef train_for_tuning_test_data(self, \n",
        "            x_train, y_train, x_test, y_test, batch_size, epochs, lr_scheduler):\n",
        "\t\tdatagen = ImageDataGenerator(\n",
        "\t\t\tfeaturewise_center=False,            # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,             # set each sample mean to 0\n",
        "            featurewise_std_normalization=False, # divide inputs by std\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,                 # apply ZCA whitening\n",
        "            rotation_range=20,                   # randomly rotate images in the range (0~180)\n",
        "            width_shift_range=0.2,               # randomly shift images horizontally\n",
        "            height_shift_range=0.2,              # randomly shift images vertically\n",
        "            zoom_range = 0.2,\n",
        "            channel_shift_range = 0.2,\n",
        "            horizontal_flip=True,                # randomly flip images\n",
        "            vertical_flip=False                  # randomly flip images\n",
        "\t\t)\n",
        "        \n",
        "    # training (validation dataはデータ拡張はしない)\n",
        "\t\tmodel_path = os.path.join(self.log_dir, self.model_file_name)\n",
        "\t\tself._target.fit_generator(\n",
        "            generator        = datagen.flow(x_train,y_train, batch_size),\n",
        "            steps_per_epoch  = x_train.shape[0] // batch_size,\n",
        "            epochs           = epochs,\n",
        "            validation_data  = ImageDataGenerator().flow(x_test,y_test, batch_size),\n",
        "\t\t\t      validation_steps = x_test.shape[0] // batch_size,\n",
        "            callbacks=[\n",
        "                LearningRateScheduler(lr_scheduler),\n",
        "                make_tensorboard(set_dir_name=self.log_dir),\n",
        "                ModelCheckpoint(model_path, save_best_only=True)\n",
        "            ],\n",
        "            verbose = self.verbose,\n",
        "            workers = 4\n",
        "        )\n",
        "\t\t\n",
        "\n",
        "class Evaluator():\n",
        "    \n",
        "    def __init__(self, result_file_path=\"./prediction_result.csv\"):\n",
        "        self.result_file_path=\"./prediction_result.csv\"\n",
        "        \n",
        "    def simple_evaluate(self, model, x_test, label):\n",
        "        print(\"start evaluation...\")\n",
        "        score = model.evaluate(x_test, y_test, verbose=1)\n",
        "        print(\"Test loss:\", score[0])\n",
        "        print(\"Test accuracy:\", score[1])\n",
        "        return score\n",
        "    \n",
        "    def tta_evaluate(self, model, x_test, label, batch_size = 2500, tta_epochs = 2):\n",
        "        print(\"batch size (TTA): \"+str(batch_size))\n",
        "        print(\"epochs (TTA): \"+str(tta_epochs))\n",
        "        tta = TTA()\n",
        "        tta_pred = tta.predict(model, x_test, batch_size, epochs = tta_epochs)\n",
        "        print(\"Test accuracy(TTA): \",end = \"\")\n",
        "        print( accuracy_score( np.argmax(tta_pred,axis = 1) , np.argmax(label,axis = 1)))\n",
        "        return tta_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCiJEpaB-HAv",
        "colab_type": "text"
      },
      "source": [
        "## 学習率減衰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4me_PZO-F2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_rate_schedule_for_Adam(epoch):\n",
        "\tlr = 0.001\n",
        "\tif(epoch >= 100): lr = 0.0002 \n",
        "\tif(epoch >= 200): lr = 0.0001\n",
        "\treturn lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKULnAcX2ilW",
        "colab_type": "text"
      },
      "source": [
        "## 実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzvig0oHnqoY",
        "colab_type": "code",
        "outputId": "380a3e67-0ecb-44f6-b494-d706c0ee157a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "# create dataset\n",
        "dataset = CIFAR10Dataset()\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "\n",
        "# create model\n",
        "model = deep_cnn(dataset.image_shape, dataset.num_classes)\n",
        "\n",
        "save_dir='/content/drive/My Drive/Colab Notebooks/Logs/deep_cnn_2/'\n",
        "\n",
        "# train the model\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=Adam(), logdir=save_dir)\n",
        "trainer.train_for_tuning_test_data(\n",
        "            x_train, y_train, x_test, y_test, batch_size=500, epochs=250, \n",
        "            lr_scheduler=learning_rate_schedule_for_Adam)\n",
        "\n",
        "\n",
        "# bestなモデルをロードする\n",
        "model = load_model(save_dir+trainer.model_file_name)\n",
        "\n",
        "# show result\n",
        "evaluator = Evaluator()\n",
        "score = evaluator.simple_evaluate(model, x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0818 14:45:35.875356 140302898800512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0818 14:45:35.908618 140302898800512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0818 14:45:35.921442 140302898800512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0818 14:45:35.973346 140302898800512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0818 14:45:35.974122 140302898800512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0818 14:45:39.248327 140302898800512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0818 14:45:39.332372 140302898800512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0818 14:45:39.341256 140302898800512 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "W0818 14:45:39.749589 140302898800512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0818 14:45:41.715444 140302898800512 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0818 14:45:43.923214 140302898800512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0818 14:45:43.924484 140302898800512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "100/100 [==============================] - 43s 432ms/step - loss: 1.9428 - acc: 0.2373 - val_loss: 5.6480 - val_acc: 0.1201\n",
            "Epoch 2/250\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 1.6016 - acc: 0.3935 - val_loss: 4.1705 - val_acc: 0.2161\n",
            "Epoch 3/250\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 1.4159 - acc: 0.4752 - val_loss: 1.5822 - val_acc: 0.4769\n",
            "Epoch 4/250\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 1.2600 - acc: 0.5451 - val_loss: 2.0890 - val_acc: 0.4709\n",
            "Epoch 5/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 1.1360 - acc: 0.5969 - val_loss: 1.3046 - val_acc: 0.6022\n",
            "Epoch 6/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 1.0410 - acc: 0.6337 - val_loss: 1.7993 - val_acc: 0.5391\n",
            "Epoch 7/250\n",
            "100/100 [==============================] - 33s 327ms/step - loss: 0.9733 - acc: 0.6576 - val_loss: 1.3872 - val_acc: 0.6206\n",
            "Epoch 8/250\n",
            "100/100 [==============================] - 33s 327ms/step - loss: 0.9200 - acc: 0.6806 - val_loss: 0.9178 - val_acc: 0.6928\n",
            "Epoch 9/250\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.8593 - acc: 0.7027 - val_loss: 1.3403 - val_acc: 0.6363\n",
            "Epoch 10/250\n",
            "100/100 [==============================] - 32s 325ms/step - loss: 0.8155 - acc: 0.7199 - val_loss: 0.7185 - val_acc: 0.7529\n",
            "Epoch 11/250\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.7722 - acc: 0.7378 - val_loss: 0.8167 - val_acc: 0.7294\n",
            "Epoch 12/250\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.7435 - acc: 0.7502 - val_loss: 0.7021 - val_acc: 0.7664\n",
            "Epoch 13/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.7079 - acc: 0.7624 - val_loss: 0.8366 - val_acc: 0.7414\n",
            "Epoch 14/250\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.6850 - acc: 0.7690 - val_loss: 0.6076 - val_acc: 0.8020\n",
            "Epoch 15/250\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.6557 - acc: 0.7808 - val_loss: 0.7796 - val_acc: 0.7630\n",
            "Epoch 16/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.6311 - acc: 0.7889 - val_loss: 0.6624 - val_acc: 0.7913\n",
            "Epoch 17/250\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.6137 - acc: 0.7941 - val_loss: 0.7014 - val_acc: 0.7886\n",
            "Epoch 18/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.5939 - acc: 0.7996 - val_loss: 0.6180 - val_acc: 0.8060\n",
            "Epoch 19/250\n",
            "100/100 [==============================] - 32s 315ms/step - loss: 0.5782 - acc: 0.8067 - val_loss: 0.6450 - val_acc: 0.7802\n",
            "Epoch 20/250\n",
            "100/100 [==============================] - 33s 326ms/step - loss: 0.5553 - acc: 0.8133 - val_loss: 0.5739 - val_acc: 0.8141\n",
            "Epoch 21/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.5496 - acc: 0.8153 - val_loss: 0.7611 - val_acc: 0.7756\n",
            "Epoch 22/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.5372 - acc: 0.8213 - val_loss: 0.5549 - val_acc: 0.8226\n",
            "Epoch 23/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.5309 - acc: 0.8216 - val_loss: 0.5036 - val_acc: 0.8421\n",
            "Epoch 24/250\n",
            "100/100 [==============================] - 33s 327ms/step - loss: 0.4989 - acc: 0.8320 - val_loss: 0.5713 - val_acc: 0.8265\n",
            "Epoch 25/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.4939 - acc: 0.8359 - val_loss: 0.4314 - val_acc: 0.8582\n",
            "Epoch 26/250\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.4903 - acc: 0.8368 - val_loss: 0.5178 - val_acc: 0.8379\n",
            "Epoch 27/250\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.4802 - acc: 0.8386 - val_loss: 0.6356 - val_acc: 0.8165\n",
            "Epoch 28/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.4667 - acc: 0.8437 - val_loss: 0.7035 - val_acc: 0.7957\n",
            "Epoch 29/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.4584 - acc: 0.8451 - val_loss: 0.4654 - val_acc: 0.8491\n",
            "Epoch 30/250\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.4499 - acc: 0.8473 - val_loss: 0.5864 - val_acc: 0.8261\n",
            "Epoch 31/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.4447 - acc: 0.8483 - val_loss: 0.4283 - val_acc: 0.8691\n",
            "Epoch 32/250\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.4325 - acc: 0.8556 - val_loss: 0.5953 - val_acc: 0.8145\n",
            "Epoch 33/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.4243 - acc: 0.8581 - val_loss: 0.4410 - val_acc: 0.8565\n",
            "Epoch 34/250\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.4232 - acc: 0.8581 - val_loss: 0.5027 - val_acc: 0.8511\n",
            "Epoch 35/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.4126 - acc: 0.8609 - val_loss: 0.3842 - val_acc: 0.8771\n",
            "Epoch 36/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.4121 - acc: 0.8634 - val_loss: 0.4738 - val_acc: 0.8622\n",
            "Epoch 37/250\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.3976 - acc: 0.8674 - val_loss: 0.5581 - val_acc: 0.8285\n",
            "Epoch 38/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.4005 - acc: 0.8669 - val_loss: 0.4806 - val_acc: 0.8560\n",
            "Epoch 39/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.3870 - acc: 0.8699 - val_loss: 0.4447 - val_acc: 0.8648\n",
            "Epoch 40/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.3812 - acc: 0.8722 - val_loss: 0.4514 - val_acc: 0.8611\n",
            "Epoch 41/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.3830 - acc: 0.8741 - val_loss: 0.4741 - val_acc: 0.8569\n",
            "Epoch 42/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.3712 - acc: 0.8749 - val_loss: 0.3978 - val_acc: 0.8728\n",
            "Epoch 43/250\n",
            "100/100 [==============================] - 32s 315ms/step - loss: 0.3669 - acc: 0.8743 - val_loss: 0.4467 - val_acc: 0.8613\n",
            "Epoch 44/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.3675 - acc: 0.8761 - val_loss: 0.5812 - val_acc: 0.8323\n",
            "Epoch 45/250\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.3665 - acc: 0.8768 - val_loss: 0.3778 - val_acc: 0.8801\n",
            "Epoch 46/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.3572 - acc: 0.8798 - val_loss: 0.3412 - val_acc: 0.8909\n",
            "Epoch 47/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.3438 - acc: 0.8830 - val_loss: 0.5101 - val_acc: 0.8582\n",
            "Epoch 48/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.3452 - acc: 0.8825 - val_loss: 0.4512 - val_acc: 0.8615\n",
            "Epoch 49/250\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.3389 - acc: 0.8857 - val_loss: 0.3587 - val_acc: 0.8860\n",
            "Epoch 50/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.3355 - acc: 0.8871 - val_loss: 0.3967 - val_acc: 0.8798\n",
            "Epoch 51/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.3266 - acc: 0.8900 - val_loss: 0.3954 - val_acc: 0.8776\n",
            "Epoch 52/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.3324 - acc: 0.8886 - val_loss: 0.3289 - val_acc: 0.8935\n",
            "Epoch 53/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.3222 - acc: 0.8913 - val_loss: 0.5793 - val_acc: 0.7806\n",
            "Epoch 54/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.3266 - acc: 0.8900 - val_loss: 0.3660 - val_acc: 0.8886\n",
            "Epoch 55/250\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.3143 - acc: 0.8942 - val_loss: 0.5125 - val_acc: 0.8529\n",
            "Epoch 56/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.3127 - acc: 0.8961 - val_loss: 0.4184 - val_acc: 0.8810\n",
            "Epoch 57/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.3143 - acc: 0.8948 - val_loss: 0.4198 - val_acc: 0.8686\n",
            "Epoch 58/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.3069 - acc: 0.8957 - val_loss: 0.3655 - val_acc: 0.8837\n",
            "Epoch 59/250\n",
            "100/100 [==============================] - 31s 313ms/step - loss: 0.3094 - acc: 0.8967 - val_loss: 0.3825 - val_acc: 0.8823\n",
            "Epoch 60/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.3078 - acc: 0.8960 - val_loss: 0.3457 - val_acc: 0.8931\n",
            "Epoch 61/250\n",
            "100/100 [==============================] - 32s 315ms/step - loss: 0.2966 - acc: 0.8988 - val_loss: 0.4518 - val_acc: 0.8625\n",
            "Epoch 62/250\n",
            "100/100 [==============================] - 32s 315ms/step - loss: 0.2954 - acc: 0.9011 - val_loss: 0.4277 - val_acc: 0.8767\n",
            "Epoch 63/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.2908 - acc: 0.9022 - val_loss: 0.3315 - val_acc: 0.8990\n",
            "Epoch 64/250\n",
            "100/100 [==============================] - 32s 315ms/step - loss: 0.2856 - acc: 0.9040 - val_loss: 0.3802 - val_acc: 0.8845\n",
            "Epoch 65/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.2890 - acc: 0.9029 - val_loss: 0.3062 - val_acc: 0.9022\n",
            "Epoch 66/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.2895 - acc: 0.9026 - val_loss: 0.3084 - val_acc: 0.9035\n",
            "Epoch 67/250\n",
            "100/100 [==============================] - 32s 315ms/step - loss: 0.2779 - acc: 0.9057 - val_loss: 0.3837 - val_acc: 0.8885\n",
            "Epoch 68/250\n",
            "100/100 [==============================] - 31s 315ms/step - loss: 0.2769 - acc: 0.9069 - val_loss: 0.3569 - val_acc: 0.8876\n",
            "Epoch 69/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.2798 - acc: 0.9062 - val_loss: 0.4358 - val_acc: 0.8725\n",
            "Epoch 70/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.2715 - acc: 0.9078 - val_loss: 0.3749 - val_acc: 0.8870\n",
            "Epoch 71/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.2710 - acc: 0.9092 - val_loss: 0.3518 - val_acc: 0.8957\n",
            "Epoch 72/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.2670 - acc: 0.9100 - val_loss: 0.3576 - val_acc: 0.8950\n",
            "Epoch 73/250\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.2667 - acc: 0.9097 - val_loss: 0.3778 - val_acc: 0.8894\n",
            "Epoch 74/250\n",
            "100/100 [==============================] - 33s 325ms/step - loss: 0.2602 - acc: 0.9116 - val_loss: 0.3607 - val_acc: 0.8910\n",
            "Epoch 75/250\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.2610 - acc: 0.9127 - val_loss: 0.3803 - val_acc: 0.8879\n",
            "Epoch 76/250\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.2560 - acc: 0.9129 - val_loss: 0.3752 - val_acc: 0.8873\n",
            "Epoch 77/250\n",
            "100/100 [==============================] - 33s 327ms/step - loss: 0.2556 - acc: 0.9131 - val_loss: 0.3401 - val_acc: 0.8980\n",
            "Epoch 78/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.2504 - acc: 0.9154 - val_loss: 0.3407 - val_acc: 0.8976\n",
            "Epoch 79/250\n",
            "100/100 [==============================] - 34s 335ms/step - loss: 0.2477 - acc: 0.9166 - val_loss: 0.3330 - val_acc: 0.8999\n",
            "Epoch 80/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.2509 - acc: 0.9152 - val_loss: 0.3029 - val_acc: 0.9055\n",
            "Epoch 81/250\n",
            "100/100 [==============================] - 35s 352ms/step - loss: 0.2456 - acc: 0.9180 - val_loss: 0.3185 - val_acc: 0.9029\n",
            "Epoch 82/250\n",
            "100/100 [==============================] - 34s 338ms/step - loss: 0.2427 - acc: 0.9183 - val_loss: 0.3377 - val_acc: 0.8962\n",
            "Epoch 83/250\n",
            "100/100 [==============================] - 34s 338ms/step - loss: 0.2456 - acc: 0.9168 - val_loss: 0.3210 - val_acc: 0.9042\n",
            "Epoch 84/250\n",
            "100/100 [==============================] - 34s 337ms/step - loss: 0.2442 - acc: 0.9167 - val_loss: 0.3098 - val_acc: 0.9038\n",
            "Epoch 85/250\n",
            "100/100 [==============================] - 34s 337ms/step - loss: 0.2369 - acc: 0.9195 - val_loss: 0.2833 - val_acc: 0.9129\n",
            "Epoch 86/250\n",
            "100/100 [==============================] - 34s 337ms/step - loss: 0.2396 - acc: 0.9197 - val_loss: 0.3442 - val_acc: 0.9006\n",
            "Epoch 87/250\n",
            "100/100 [==============================] - 35s 348ms/step - loss: 0.2348 - acc: 0.9202 - val_loss: 0.3680 - val_acc: 0.8944\n",
            "Epoch 88/250\n",
            "100/100 [==============================] - 34s 339ms/step - loss: 0.2341 - acc: 0.9211 - val_loss: 0.3209 - val_acc: 0.9029\n",
            "Epoch 89/250\n",
            "100/100 [==============================] - 34s 338ms/step - loss: 0.2301 - acc: 0.9226 - val_loss: 0.3367 - val_acc: 0.8960\n",
            "Epoch 90/250\n",
            "100/100 [==============================] - 34s 337ms/step - loss: 0.2303 - acc: 0.9219 - val_loss: 0.2907 - val_acc: 0.9114\n",
            "Epoch 91/250\n",
            "100/100 [==============================] - 34s 339ms/step - loss: 0.2253 - acc: 0.9234 - val_loss: 0.3422 - val_acc: 0.9025\n",
            "Epoch 92/250\n",
            "100/100 [==============================] - 34s 337ms/step - loss: 0.2277 - acc: 0.9224 - val_loss: 0.3200 - val_acc: 0.9054\n",
            "Epoch 93/250\n",
            "100/100 [==============================] - 34s 338ms/step - loss: 0.2211 - acc: 0.9258 - val_loss: 0.4455 - val_acc: 0.8877\n",
            "Epoch 94/250\n",
            "100/100 [==============================] - 34s 336ms/step - loss: 0.2290 - acc: 0.9229 - val_loss: 0.3378 - val_acc: 0.8982\n",
            "Epoch 95/250\n",
            "100/100 [==============================] - 34s 336ms/step - loss: 0.2233 - acc: 0.9247 - val_loss: 0.3399 - val_acc: 0.9010\n",
            "Epoch 96/250\n",
            "100/100 [==============================] - 34s 336ms/step - loss: 0.2166 - acc: 0.9252 - val_loss: 0.3342 - val_acc: 0.9048\n",
            "Epoch 97/250\n",
            "100/100 [==============================] - 33s 335ms/step - loss: 0.2186 - acc: 0.9262 - val_loss: 0.3149 - val_acc: 0.9036\n",
            "Epoch 98/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.2128 - acc: 0.9287 - val_loss: 0.4183 - val_acc: 0.8870\n",
            "Epoch 99/250\n",
            "100/100 [==============================] - 34s 339ms/step - loss: 0.2104 - acc: 0.9294 - val_loss: 0.2918 - val_acc: 0.9115\n",
            "Epoch 100/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.2154 - acc: 0.9263 - val_loss: 0.3310 - val_acc: 0.9014\n",
            "Epoch 101/250\n",
            "100/100 [==============================] - 34s 336ms/step - loss: 0.1794 - acc: 0.9394 - val_loss: 0.2736 - val_acc: 0.9200\n",
            "Epoch 102/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1619 - acc: 0.9454 - val_loss: 0.2767 - val_acc: 0.9214\n",
            "Epoch 103/250\n",
            "100/100 [==============================] - 34s 335ms/step - loss: 0.1542 - acc: 0.9473 - val_loss: 0.2467 - val_acc: 0.9266\n",
            "Epoch 104/250\n",
            "100/100 [==============================] - 34s 336ms/step - loss: 0.1490 - acc: 0.9490 - val_loss: 0.2749 - val_acc: 0.9220\n",
            "Epoch 105/250\n",
            "100/100 [==============================] - 33s 332ms/step - loss: 0.1506 - acc: 0.9473 - val_loss: 0.2534 - val_acc: 0.9275\n",
            "Epoch 106/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1429 - acc: 0.9516 - val_loss: 0.2429 - val_acc: 0.9295\n",
            "Epoch 107/250\n",
            "100/100 [==============================] - 34s 335ms/step - loss: 0.1422 - acc: 0.9512 - val_loss: 0.2693 - val_acc: 0.9248\n",
            "Epoch 108/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.1441 - acc: 0.9520 - val_loss: 0.2866 - val_acc: 0.9222\n",
            "Epoch 109/250\n",
            "100/100 [==============================] - 33s 332ms/step - loss: 0.1422 - acc: 0.9505 - val_loss: 0.2571 - val_acc: 0.9257\n",
            "Epoch 110/250\n",
            "100/100 [==============================] - 34s 337ms/step - loss: 0.1388 - acc: 0.9524 - val_loss: 0.2626 - val_acc: 0.9261\n",
            "Epoch 111/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.1370 - acc: 0.9538 - val_loss: 0.2918 - val_acc: 0.9217\n",
            "Epoch 112/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.1341 - acc: 0.9537 - val_loss: 0.2741 - val_acc: 0.9220\n",
            "Epoch 113/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1364 - acc: 0.9528 - val_loss: 0.2789 - val_acc: 0.9215\n",
            "Epoch 114/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1334 - acc: 0.9549 - val_loss: 0.2842 - val_acc: 0.9227\n",
            "Epoch 115/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1298 - acc: 0.9552 - val_loss: 0.2621 - val_acc: 0.9270\n",
            "Epoch 116/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.1366 - acc: 0.9538 - val_loss: 0.2492 - val_acc: 0.9278\n",
            "Epoch 117/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.1274 - acc: 0.9564 - val_loss: 0.2720 - val_acc: 0.9245\n",
            "Epoch 118/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1334 - acc: 0.9535 - val_loss: 0.2572 - val_acc: 0.9280\n",
            "Epoch 119/250\n",
            "100/100 [==============================] - 33s 335ms/step - loss: 0.1288 - acc: 0.9556 - val_loss: 0.2734 - val_acc: 0.9227\n",
            "Epoch 120/250\n",
            "100/100 [==============================] - 34s 336ms/step - loss: 0.1303 - acc: 0.9557 - val_loss: 0.2820 - val_acc: 0.9237\n",
            "Epoch 121/250\n",
            "100/100 [==============================] - 33s 332ms/step - loss: 0.1308 - acc: 0.9554 - val_loss: 0.2714 - val_acc: 0.9220\n",
            "Epoch 122/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1297 - acc: 0.9562 - val_loss: 0.2759 - val_acc: 0.9222\n",
            "Epoch 123/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1258 - acc: 0.9567 - val_loss: 0.2745 - val_acc: 0.9275\n",
            "Epoch 124/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1265 - acc: 0.9562 - val_loss: 0.2700 - val_acc: 0.9257\n",
            "Epoch 125/250\n",
            "100/100 [==============================] - 34s 335ms/step - loss: 0.1236 - acc: 0.9582 - val_loss: 0.2724 - val_acc: 0.9243\n",
            "Epoch 126/250\n",
            "100/100 [==============================] - 33s 332ms/step - loss: 0.1251 - acc: 0.9571 - val_loss: 0.2671 - val_acc: 0.9267\n",
            "Epoch 127/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1239 - acc: 0.9573 - val_loss: 0.2796 - val_acc: 0.9251\n",
            "Epoch 128/250\n",
            "100/100 [==============================] - 34s 335ms/step - loss: 0.1233 - acc: 0.9579 - val_loss: 0.2625 - val_acc: 0.9279\n",
            "Epoch 129/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1181 - acc: 0.9591 - val_loss: 0.2858 - val_acc: 0.9243\n",
            "Epoch 130/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1265 - acc: 0.9573 - val_loss: 0.2819 - val_acc: 0.9228\n",
            "Epoch 131/250\n",
            "100/100 [==============================] - 33s 332ms/step - loss: 0.1215 - acc: 0.9580 - val_loss: 0.3016 - val_acc: 0.9204\n",
            "Epoch 132/250\n",
            "100/100 [==============================] - 33s 335ms/step - loss: 0.1192 - acc: 0.9583 - val_loss: 0.2824 - val_acc: 0.9261\n",
            "Epoch 133/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.1219 - acc: 0.9574 - val_loss: 0.2707 - val_acc: 0.9277\n",
            "Epoch 134/250\n",
            "100/100 [==============================] - 33s 335ms/step - loss: 0.1187 - acc: 0.9587 - val_loss: 0.2805 - val_acc: 0.9250\n",
            "Epoch 135/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1171 - acc: 0.9599 - val_loss: 0.2967 - val_acc: 0.9249\n",
            "Epoch 136/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.1225 - acc: 0.9580 - val_loss: 0.2751 - val_acc: 0.9263\n",
            "Epoch 137/250\n",
            "100/100 [==============================] - 34s 337ms/step - loss: 0.1133 - acc: 0.9616 - val_loss: 0.2825 - val_acc: 0.9242\n",
            "Epoch 138/250\n",
            "100/100 [==============================] - 34s 336ms/step - loss: 0.1156 - acc: 0.9599 - val_loss: 0.3126 - val_acc: 0.9219\n",
            "Epoch 139/250\n",
            "100/100 [==============================] - 34s 338ms/step - loss: 0.1203 - acc: 0.9590 - val_loss: 0.2827 - val_acc: 0.9254\n",
            "Epoch 140/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.1144 - acc: 0.9611 - val_loss: 0.2825 - val_acc: 0.9260\n",
            "Epoch 141/250\n",
            "100/100 [==============================] - 34s 338ms/step - loss: 0.1110 - acc: 0.9613 - val_loss: 0.2760 - val_acc: 0.9284\n",
            "Epoch 142/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1152 - acc: 0.9600 - val_loss: 0.2900 - val_acc: 0.9249\n",
            "Epoch 143/250\n",
            "100/100 [==============================] - 34s 336ms/step - loss: 0.1137 - acc: 0.9613 - val_loss: 0.3063 - val_acc: 0.9206\n",
            "Epoch 144/250\n",
            "100/100 [==============================] - 34s 336ms/step - loss: 0.1103 - acc: 0.9624 - val_loss: 0.2945 - val_acc: 0.9264\n",
            "Epoch 145/250\n",
            "100/100 [==============================] - 33s 335ms/step - loss: 0.1111 - acc: 0.9618 - val_loss: 0.3044 - val_acc: 0.9217\n",
            "Epoch 146/250\n",
            "100/100 [==============================] - 33s 335ms/step - loss: 0.1093 - acc: 0.9621 - val_loss: 0.2820 - val_acc: 0.9260\n",
            "Epoch 147/250\n",
            "100/100 [==============================] - 34s 337ms/step - loss: 0.1147 - acc: 0.9604 - val_loss: 0.2935 - val_acc: 0.9240\n",
            "Epoch 148/250\n",
            "100/100 [==============================] - 33s 334ms/step - loss: 0.1086 - acc: 0.9625 - val_loss: 0.2856 - val_acc: 0.9250\n",
            "Epoch 149/250\n",
            "100/100 [==============================] - 34s 335ms/step - loss: 0.1094 - acc: 0.9630 - val_loss: 0.2865 - val_acc: 0.9275\n",
            "Epoch 150/250\n",
            "100/100 [==============================] - 33s 332ms/step - loss: 0.1080 - acc: 0.9627 - val_loss: 0.2886 - val_acc: 0.9253\n",
            "Epoch 151/250\n",
            "100/100 [==============================] - 33s 332ms/step - loss: 0.1124 - acc: 0.9614 - val_loss: 0.2764 - val_acc: 0.9246\n",
            "Epoch 152/250\n",
            "100/100 [==============================] - 33s 331ms/step - loss: 0.1127 - acc: 0.9619 - val_loss: 0.2788 - val_acc: 0.9259\n",
            "Epoch 153/250\n",
            "100/100 [==============================] - 35s 345ms/step - loss: 0.1055 - acc: 0.9636 - val_loss: 0.2825 - val_acc: 0.9248\n",
            "Epoch 154/250\n",
            "100/100 [==============================] - 33s 333ms/step - loss: 0.1031 - acc: 0.9646 - val_loss: 0.2831 - val_acc: 0.9270\n",
            "Epoch 155/250\n",
            "100/100 [==============================] - 33s 327ms/step - loss: 0.1121 - acc: 0.9623 - val_loss: 0.3039 - val_acc: 0.9220\n",
            "Epoch 156/250\n",
            "100/100 [==============================] - 33s 331ms/step - loss: 0.1062 - acc: 0.9631 - val_loss: 0.2962 - val_acc: 0.9255\n",
            "Epoch 157/250\n",
            "100/100 [==============================] - 33s 325ms/step - loss: 0.1057 - acc: 0.9648 - val_loss: 0.2779 - val_acc: 0.9255\n",
            "Epoch 158/250\n",
            "100/100 [==============================] - 33s 325ms/step - loss: 0.1088 - acc: 0.9619 - val_loss: 0.2752 - val_acc: 0.9247\n",
            "Epoch 159/250\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.1080 - acc: 0.9631 - val_loss: 0.2880 - val_acc: 0.9237\n",
            "Epoch 160/250\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.1057 - acc: 0.9645 - val_loss: 0.2834 - val_acc: 0.9253\n",
            "Epoch 161/250\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.0999 - acc: 0.9655 - val_loss: 0.2854 - val_acc: 0.9272\n",
            "Epoch 162/250\n",
            "100/100 [==============================] - 32s 325ms/step - loss: 0.1075 - acc: 0.9646 - val_loss: 0.2816 - val_acc: 0.9238\n",
            "Epoch 163/250\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.1045 - acc: 0.9646 - val_loss: 0.2706 - val_acc: 0.9288\n",
            "Epoch 164/250\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.1053 - acc: 0.9638 - val_loss: 0.2849 - val_acc: 0.9259\n",
            "Epoch 165/250\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.1020 - acc: 0.9653 - val_loss: 0.2908 - val_acc: 0.9238\n",
            "Epoch 166/250\n",
            "100/100 [==============================] - 33s 326ms/step - loss: 0.1026 - acc: 0.9652 - val_loss: 0.2873 - val_acc: 0.9273\n",
            "Epoch 167/250\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.0994 - acc: 0.9654 - val_loss: 0.2804 - val_acc: 0.9264\n",
            "Epoch 168/250\n",
            "100/100 [==============================] - 33s 327ms/step - loss: 0.1016 - acc: 0.9655 - val_loss: 0.2709 - val_acc: 0.9292\n",
            "Epoch 169/250\n",
            "100/100 [==============================] - 33s 329ms/step - loss: 0.1032 - acc: 0.9653 - val_loss: 0.3214 - val_acc: 0.9187\n",
            "Epoch 170/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.0988 - acc: 0.9660 - val_loss: 0.3103 - val_acc: 0.9221\n",
            "Epoch 171/250\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.1017 - acc: 0.9655 - val_loss: 0.3024 - val_acc: 0.9237\n",
            "Epoch 172/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.1001 - acc: 0.9654 - val_loss: 0.3223 - val_acc: 0.9207\n",
            "Epoch 173/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.0982 - acc: 0.9667 - val_loss: 0.2825 - val_acc: 0.9282\n",
            "Epoch 174/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.0975 - acc: 0.9670 - val_loss: 0.2888 - val_acc: 0.9282\n",
            "Epoch 175/250\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.1004 - acc: 0.9651 - val_loss: 0.2940 - val_acc: 0.9261\n",
            "Epoch 176/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.0990 - acc: 0.9660 - val_loss: 0.2978 - val_acc: 0.9260\n",
            "Epoch 177/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.0994 - acc: 0.9651 - val_loss: 0.3008 - val_acc: 0.9244\n",
            "Epoch 178/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.0995 - acc: 0.9659 - val_loss: 0.2814 - val_acc: 0.9263\n",
            "Epoch 179/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.1011 - acc: 0.9658 - val_loss: 0.2875 - val_acc: 0.9246\n",
            "Epoch 180/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0958 - acc: 0.9677 - val_loss: 0.2845 - val_acc: 0.9259\n",
            "Epoch 181/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.0970 - acc: 0.9675 - val_loss: 0.2762 - val_acc: 0.9261\n",
            "Epoch 182/250\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.0998 - acc: 0.9662 - val_loss: 0.2776 - val_acc: 0.9287\n",
            "Epoch 183/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.0953 - acc: 0.9678 - val_loss: 0.2907 - val_acc: 0.9264\n",
            "Epoch 184/250\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.0949 - acc: 0.9671 - val_loss: 0.2732 - val_acc: 0.9289\n",
            "Epoch 185/250\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.1006 - acc: 0.9664 - val_loss: 0.3145 - val_acc: 0.9219\n",
            "Epoch 186/250\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.0964 - acc: 0.9667 - val_loss: 0.2775 - val_acc: 0.9273\n",
            "Epoch 187/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.0959 - acc: 0.9686 - val_loss: 0.3248 - val_acc: 0.9173\n",
            "Epoch 188/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.0957 - acc: 0.9681 - val_loss: 0.3018 - val_acc: 0.9233\n",
            "Epoch 189/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.0940 - acc: 0.9676 - val_loss: 0.2829 - val_acc: 0.9278\n",
            "Epoch 190/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0953 - acc: 0.9669 - val_loss: 0.3146 - val_acc: 0.9212\n",
            "Epoch 191/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0948 - acc: 0.9672 - val_loss: 0.3014 - val_acc: 0.9227\n",
            "Epoch 192/250\n",
            "100/100 [==============================] - 31s 313ms/step - loss: 0.0936 - acc: 0.9674 - val_loss: 0.3029 - val_acc: 0.9234\n",
            "Epoch 193/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.0961 - acc: 0.9676 - val_loss: 0.2842 - val_acc: 0.9271\n",
            "Epoch 194/250\n",
            "100/100 [==============================] - 31s 315ms/step - loss: 0.0943 - acc: 0.9669 - val_loss: 0.2841 - val_acc: 0.9271\n",
            "Epoch 195/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.0977 - acc: 0.9668 - val_loss: 0.2998 - val_acc: 0.9227\n",
            "Epoch 196/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.0938 - acc: 0.9676 - val_loss: 0.2916 - val_acc: 0.9245\n",
            "Epoch 197/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.0927 - acc: 0.9689 - val_loss: 0.2875 - val_acc: 0.9253\n",
            "Epoch 198/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0912 - acc: 0.9691 - val_loss: 0.2944 - val_acc: 0.9265\n",
            "Epoch 199/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.0931 - acc: 0.9679 - val_loss: 0.2992 - val_acc: 0.9244\n",
            "Epoch 200/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.0954 - acc: 0.9676 - val_loss: 0.2992 - val_acc: 0.9250\n",
            "Epoch 201/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.0903 - acc: 0.9689 - val_loss: 0.2868 - val_acc: 0.9271\n",
            "Epoch 202/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0880 - acc: 0.9698 - val_loss: 0.3017 - val_acc: 0.9254\n",
            "Epoch 203/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0837 - acc: 0.9717 - val_loss: 0.2936 - val_acc: 0.9282\n",
            "Epoch 204/250\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.0848 - acc: 0.9714 - val_loss: 0.3047 - val_acc: 0.9260\n",
            "Epoch 205/250\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.0799 - acc: 0.9729 - val_loss: 0.3064 - val_acc: 0.9268\n",
            "Epoch 206/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.0815 - acc: 0.9726 - val_loss: 0.2785 - val_acc: 0.9306\n",
            "Epoch 207/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.0850 - acc: 0.9710 - val_loss: 0.2830 - val_acc: 0.9311\n",
            "Epoch 208/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.0851 - acc: 0.9707 - val_loss: 0.2968 - val_acc: 0.9260\n",
            "Epoch 209/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0805 - acc: 0.9725 - val_loss: 0.3109 - val_acc: 0.9247\n",
            "Epoch 210/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.0825 - acc: 0.9714 - val_loss: 0.2919 - val_acc: 0.9282\n",
            "Epoch 211/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.0820 - acc: 0.9719 - val_loss: 0.3014 - val_acc: 0.9290\n",
            "Epoch 212/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.0806 - acc: 0.9718 - val_loss: 0.3053 - val_acc: 0.9267\n",
            "Epoch 213/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.0802 - acc: 0.9729 - val_loss: 0.3020 - val_acc: 0.9252\n",
            "Epoch 214/250\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.0804 - acc: 0.9730 - val_loss: 0.3082 - val_acc: 0.9265\n",
            "Epoch 215/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.0820 - acc: 0.9708 - val_loss: 0.2986 - val_acc: 0.9283\n",
            "Epoch 216/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.0799 - acc: 0.9726 - val_loss: 0.2919 - val_acc: 0.9300\n",
            "Epoch 217/250\n",
            "100/100 [==============================] - 31s 315ms/step - loss: 0.0789 - acc: 0.9728 - val_loss: 0.2961 - val_acc: 0.9282\n",
            "Epoch 218/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0783 - acc: 0.9736 - val_loss: 0.3024 - val_acc: 0.9262\n",
            "Epoch 219/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.0814 - acc: 0.9724 - val_loss: 0.3038 - val_acc: 0.9254\n",
            "Epoch 220/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.0813 - acc: 0.9723 - val_loss: 0.2928 - val_acc: 0.9278\n",
            "Epoch 221/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.0812 - acc: 0.9716 - val_loss: 0.2907 - val_acc: 0.9289\n",
            "Epoch 222/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0808 - acc: 0.9733 - val_loss: 0.3083 - val_acc: 0.9260\n",
            "Epoch 223/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.0762 - acc: 0.9743 - val_loss: 0.3004 - val_acc: 0.9278\n",
            "Epoch 224/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0786 - acc: 0.9726 - val_loss: 0.3029 - val_acc: 0.9282\n",
            "Epoch 225/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.0771 - acc: 0.9731 - val_loss: 0.3181 - val_acc: 0.9249\n",
            "Epoch 226/250\n",
            "100/100 [==============================] - 32s 315ms/step - loss: 0.0781 - acc: 0.9734 - val_loss: 0.2953 - val_acc: 0.9287\n",
            "Epoch 227/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0793 - acc: 0.9726 - val_loss: 0.2948 - val_acc: 0.9291\n",
            "Epoch 228/250\n",
            "100/100 [==============================] - 31s 315ms/step - loss: 0.0788 - acc: 0.9726 - val_loss: 0.2920 - val_acc: 0.9270\n",
            "Epoch 229/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0768 - acc: 0.9731 - val_loss: 0.2896 - val_acc: 0.9307\n",
            "Epoch 230/250\n",
            "100/100 [==============================] - 31s 315ms/step - loss: 0.0797 - acc: 0.9732 - val_loss: 0.2904 - val_acc: 0.9288\n",
            "Epoch 231/250\n",
            "100/100 [==============================] - 31s 313ms/step - loss: 0.0778 - acc: 0.9745 - val_loss: 0.2974 - val_acc: 0.9295\n",
            "Epoch 232/250\n",
            "100/100 [==============================] - 31s 313ms/step - loss: 0.0756 - acc: 0.9742 - val_loss: 0.2959 - val_acc: 0.9295\n",
            "Epoch 233/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0741 - acc: 0.9739 - val_loss: 0.2910 - val_acc: 0.9300\n",
            "Epoch 234/250\n",
            "100/100 [==============================] - 31s 313ms/step - loss: 0.0745 - acc: 0.9737 - val_loss: 0.3103 - val_acc: 0.9261\n",
            "Epoch 235/250\n",
            "100/100 [==============================] - 31s 311ms/step - loss: 0.0773 - acc: 0.9734 - val_loss: 0.2926 - val_acc: 0.9299\n",
            "Epoch 236/250\n",
            "100/100 [==============================] - 31s 311ms/step - loss: 0.0793 - acc: 0.9736 - val_loss: 0.2850 - val_acc: 0.9292\n",
            "Epoch 237/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0767 - acc: 0.9743 - val_loss: 0.3020 - val_acc: 0.9267\n",
            "Epoch 238/250\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.0782 - acc: 0.9732 - val_loss: 0.2940 - val_acc: 0.9293\n",
            "Epoch 239/250\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.0793 - acc: 0.9723 - val_loss: 0.2913 - val_acc: 0.9303\n",
            "Epoch 240/250\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.0751 - acc: 0.9744 - val_loss: 0.3059 - val_acc: 0.9253\n",
            "Epoch 241/250\n",
            "100/100 [==============================] - 31s 315ms/step - loss: 0.0747 - acc: 0.9745 - val_loss: 0.2963 - val_acc: 0.9278\n",
            "Epoch 242/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0756 - acc: 0.9729 - val_loss: 0.3040 - val_acc: 0.9284\n",
            "Epoch 243/250\n",
            "100/100 [==============================] - 31s 312ms/step - loss: 0.0752 - acc: 0.9745 - val_loss: 0.3031 - val_acc: 0.9269\n",
            "Epoch 244/250\n",
            "100/100 [==============================] - 32s 316ms/step - loss: 0.0737 - acc: 0.9752 - val_loss: 0.2888 - val_acc: 0.9311\n",
            "Epoch 245/250\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.0739 - acc: 0.9750 - val_loss: 0.2986 - val_acc: 0.9296\n",
            "Epoch 246/250\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.0728 - acc: 0.9749 - val_loss: 0.3025 - val_acc: 0.9300\n",
            "Epoch 247/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.0756 - acc: 0.9743 - val_loss: 0.2825 - val_acc: 0.9308\n",
            "Epoch 248/250\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.0735 - acc: 0.9748 - val_loss: 0.3102 - val_acc: 0.9260\n",
            "Epoch 249/250\n",
            "100/100 [==============================] - 31s 313ms/step - loss: 0.0737 - acc: 0.9746 - val_loss: 0.2955 - val_acc: 0.9297\n",
            "Epoch 250/250\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 0.0771 - acc: 0.9738 - val_loss: 0.3047 - val_acc: 0.9272\n",
            "start evaluation...\n",
            "10000/10000 [==============================] - 3s 329us/step\n",
            "Test loss: 0.24294587227776646\n",
            "Test accuracy: 0.9295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mywGzeIugMUf",
        "colab_type": "text"
      },
      "source": [
        "##  Test Time Augmentation（TTA）を用いた推論 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkHjWCv1gLi6",
        "colab_type": "code",
        "outputId": "f7a43104-83dc-46b3-ca52-7697c55f6054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "class TTA:\n",
        "    \n",
        "    #test_time_augmentation\n",
        "    #batch_sizeは，test_sizeの約数!!!\n",
        "    def predict(self, model, x_test, batch_size ,epochs = 10):\n",
        "        \n",
        "        # Augmentation用generatorによるデータセットの作成\n",
        "        data_flow = self.generator(x_test, batch_size)\n",
        "        \n",
        "        test_size = x_test.shape[0]\n",
        "        pred = np.zeros(shape = (test_size,10), dtype = float)\n",
        "        \n",
        "        step_per_epoch = test_size //batch_size\n",
        "        for epoch in range(epochs):\n",
        "            print( 'epoch: ' + str(epoch+1)+'/'+str(epochs))\n",
        "            for step in range(step_per_epoch):\n",
        "                #print( 'step: ' + str(step+1)+'/'+str(step_per_epoch))\n",
        "                sta = batch_size * step\n",
        "                end = sta + batch_size\n",
        "                tmp_x = data_flow.__next__()\n",
        "                pred[sta:end] += model.predict(tmp_x)        \n",
        "        return pred / epochs\n",
        "    \n",
        "    \n",
        "    def generator(self, x_test,batch_size):\n",
        "        return ImageDataGenerator(\n",
        "                    rotation_range = 20,\n",
        "                    horizontal_flip = True,\n",
        "                    height_shift_range = 0.2,\n",
        "                    width_shift_range = 0.2,\n",
        "                    zoom_range = 0.2,\n",
        "                    channel_shift_range = 0.2\n",
        "                ).flow(x_test,batch_size = batch_size,shuffle = False)\n",
        "\n",
        "      \n",
        "# show result\n",
        "evaluator = Evaluator()\n",
        "score = evaluator.tta_evaluate(model, x_test, y_test, batch_size = 500, tta_epochs = 50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch size (TTA): 500\n",
            "epochs (TTA): 50\n",
            "epoch: 1/50\n",
            "epoch: 2/50\n",
            "epoch: 3/50\n",
            "epoch: 4/50\n",
            "epoch: 5/50\n",
            "epoch: 6/50\n",
            "epoch: 7/50\n",
            "epoch: 8/50\n",
            "epoch: 9/50\n",
            "epoch: 10/50\n",
            "epoch: 11/50\n",
            "epoch: 12/50\n",
            "epoch: 13/50\n",
            "epoch: 14/50\n",
            "epoch: 15/50\n",
            "epoch: 16/50\n",
            "epoch: 17/50\n",
            "epoch: 18/50\n",
            "epoch: 19/50\n",
            "epoch: 20/50\n",
            "epoch: 21/50\n",
            "epoch: 22/50\n",
            "epoch: 23/50\n",
            "epoch: 24/50\n",
            "epoch: 25/50\n",
            "epoch: 26/50\n",
            "epoch: 27/50\n",
            "epoch: 28/50\n",
            "epoch: 29/50\n",
            "epoch: 30/50\n",
            "epoch: 31/50\n",
            "epoch: 32/50\n",
            "epoch: 33/50\n",
            "epoch: 34/50\n",
            "epoch: 35/50\n",
            "epoch: 36/50\n",
            "epoch: 37/50\n",
            "epoch: 38/50\n",
            "epoch: 39/50\n",
            "epoch: 40/50\n",
            "epoch: 41/50\n",
            "epoch: 42/50\n",
            "epoch: 43/50\n",
            "epoch: 44/50\n",
            "epoch: 45/50\n",
            "epoch: 46/50\n",
            "epoch: 47/50\n",
            "epoch: 48/50\n",
            "epoch: 49/50\n",
            "epoch: 50/50\n",
            "Test accuracy(TTA): 0.9386\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}