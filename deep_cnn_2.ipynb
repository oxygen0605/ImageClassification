{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_cnn_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oxygen0605/ImageClassification/blob/master/deep_cnn_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj4ebRdWOMpm",
        "colab_type": "text"
      },
      "source": [
        "# Google Colaboratory環境の初期設定\n",
        "\n",
        "## Google Driveにマウントしてマシンスペックを出力"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIgX8qtmOOY9",
        "colab_type": "code",
        "outputId": "3ade768f-6048-4a41-d7c0-88fd5143d050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!nvidia-smi > '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'\n",
        "!cat /proc/driver/nvidia/gpus/0000:00:04.0/information >> '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'\n",
        "!cat /etc/issue >> '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'\n",
        "!cat /proc/cpuinfo >> '/content/drive/My Drive/Colab Notebooks/Logs/machine_spec.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5whB14f2LCXO",
        "colab_type": "code",
        "outputId": "5e9d06df-8d1e-47a0-9fc3-7f2c3970ddf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "!ls -al"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Aug 20 16:01 .\n",
            "drwxr-xr-x 1 root root 4096 Aug 20 15:51 ..\n",
            "drwxr-xr-x 1 root root 4096 Aug 13 16:04 .config\n",
            "drwx------ 3 root root 4096 Aug 20 16:01 drive\n",
            "drwxr-xr-x 1 root root 4096 Aug  2 16:06 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45yQPr6Lltrj",
        "colab_type": "text"
      },
      "source": [
        "# Deep CNN (CIFAR-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur3z5pdimJdX",
        "colab_type": "text"
      },
      "source": [
        "## モデルの生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CLdMi7Wlzwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Dense, BatchNormalization\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Activation, Flatten\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras import regularizers\n",
        "\n",
        "def deep_cnn(input_shape, num_classes):\n",
        "    inputs = Input(shape = input_shape)\n",
        "    \n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(inputs)\n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(512,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = Conv2D(512,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = Dense(1024,activation = \"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1024,activation = \"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    y  = Dense(num_classes, activation = \"softmax\")(x)\n",
        "\n",
        "    return Model(input = inputs, output = y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygmn8vNImOV-",
        "colab_type": "text"
      },
      "source": [
        "## CIFAR10 データセットの用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JELm-0dimTvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "class CIFAR10Dataset():\n",
        "\tdef __init__(self):\n",
        "\t\tself.image_shape = (32, 32, 3)\n",
        "\t\tself.num_classes = 10\n",
        "\t\t\n",
        "\tdef preprocess(self, data, label_data=False):\n",
        "\t\tif label_data:\n",
        "\t\t\t# conver class number to one-hot vector\n",
        "\t\t\tdata = keras.utils.to_categorical(data, self.num_classes)\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tdata = data.astype(\"float32\")\n",
        "\t\t\tdata /= 255 #convert the value to 0 ~ 1 scale\n",
        "\t\t\tshape = (data.shape[0],) + self.image_shape\n",
        "\t\t\tdata = data.reshape(shape)\n",
        "\t\t\t\n",
        "\t\treturn data\n",
        "\t\n",
        "\tdef get_batch(self):\n",
        "\t\t# x: data, y: lebel\n",
        "\t\t(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\t\t\n",
        "\t\tx_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
        "\t\ty_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
        "\t\t\t\t\t [y_train, y_test]]\n",
        "\t\t\n",
        "\t\treturn x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pawbuqkSPkg",
        "colab_type": "text"
      },
      "source": [
        "## TensorBoard用のログファイル生成関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEgHJML0SZdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "from time import gmtime, strftime\n",
        "from keras.callbacks import TensorBoard\n",
        "import os\n",
        "\n",
        "\n",
        "def make_tensorboard(set_dir_name=''):\n",
        "    tictoc = strftime(\"%a_%d_%b_%Y_%H_%M_%S\", gmtime())\n",
        "    directory_name = tictoc\n",
        "    log_dir = set_dir_name + '_' + directory_name\n",
        "    os.mkdir(log_dir)\n",
        "    tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, )\n",
        "    return tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtlErnk9tL4z",
        "colab_type": "text"
      },
      "source": [
        "## ImageDataGeneratorクラスの拡張\n",
        "random crop\n",
        "mix up\n",
        "cutout\n",
        "を実装する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMROWCFSmE5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "class ImageDataGeneratorEX(ImageDataGenerator):\n",
        "\tdef __init__(self,\n",
        "               featurewise_center=False,\n",
        "               samplewise_center=False, \n",
        "               featurewise_std_normalization=False,\n",
        "               samplewise_std_normalization=False,\n",
        "               zca_whitening=False,\n",
        "               zca_epsilon=1e-06,\n",
        "               rotation_range=0.0,\n",
        "               width_shift_range=0.0,\n",
        "               height_shift_range=0.0,\n",
        "               brightness_range=None,\n",
        "               shear_range=0.0,\n",
        "               zoom_range=0.0, \n",
        "               channel_shift_range=0.0,\n",
        "               fill_mode='nearest',\n",
        "               cval=0.0,\n",
        "               horizontal_flip=False, \n",
        "               vertical_flip=False,\n",
        "               rescale=None,\n",
        "               preprocessing_function=None,\n",
        "               data_format=None,\n",
        "               validation_split=0.0, \n",
        "               random_crop=None,    # a new parameter for random crop\n",
        "               mix_up_alpha=0.0,    # a new parameter for mix up\n",
        "               cutout_mask_size=0   # a new parameter for cutout\n",
        "              ):\n",
        "    \n",
        "\t\t# 親クラスのコンストラクタ\n",
        "\t\tsuper().__init__(featurewise_center, samplewise_center, featurewise_std_normalization, samplewise_std_normalization, zca_whitening, zca_epsilon, rotation_range, width_shift_range, height_shift_range, brightness_range, shear_range, zoom_range, channel_shift_range, fill_mode, cval, horizontal_flip, vertical_flip, rescale, preprocessing_function, data_format, validation_split)\n",
        "\t\t# 拡張処理のパラメーター\n",
        "\t\t # Mix-up\n",
        "\t\tassert mix_up_alpha >= 0.0\n",
        "\t\tself.mix_up_alpha = mix_up_alpha\n",
        "\t\t# Random Crop\n",
        "\t\tassert random_crop == None or len(random_crop) == 2\n",
        "\t\tself.random_crop_size = random_crop\n",
        "\t\tself.cutout_mask_size = cutout_mask_size\n",
        "    \n",
        "\t# ランダムクロップ\n",
        "    # 参考 https://jkjung-avt.github.io/keras-image-cropping/\n",
        "\tdef random_crop(self, original_img):\n",
        "        # Note: image_data_format is 'channel_last'\n",
        "\t\tassert original_img.shape[2] == 3\n",
        "\t\tif original_img.shape[0] < self.random_crop_size[0] or original_img.shape[1] < self.random_crop_size[1]:\n",
        "\t\t\traise ValueError(f\"Invalid random_crop_size : original = {original_img.shape}, crop_size = {self.random_crop_size}\")\n",
        "\t\theight, width = original_img.shape[0], original_img.shape[1]\n",
        "\t\tdy, dx = self.random_crop_size\n",
        "\t\tx = np.random.randint(0, width - dx + 1)\n",
        "\t\ty = np.random.randint(0, height - dy + 1)\n",
        "\t\treturn original_img[y:(y+dy), x:(x+dx), :]\n",
        "\n",
        "    # Mix-up\n",
        "    # 参考 https://qiita.com/yu4u/items/70aa007346ec73b7ff05\n",
        "\tdef mix_up(self, X1, y1, X2, y2):\n",
        "\t\tassert X1.shape[0] == y1.shape[0] == X2.shape[0] == y2.shape[0]\n",
        "\t\tbatch_size = X1.shape[0]\n",
        "\t\tl = np.random.beta(self.mix_up_alpha, self.mix_up_alpha, batch_size)\n",
        "\t\tX_l = l.reshape(batch_size, 1, 1, 1)\n",
        "\t\ty_l = l.reshape(batch_size, 1)\n",
        "\t\tX = X1 * X_l + X2 * (1-X_l)\n",
        "\t\ty = y1 * y_l + y2 * (1-y_l)\n",
        "\t\treturn X, y\n",
        "    \n",
        "\tdef cutout(self, x, y):\n",
        "\t\treturn np.array(list(map(self._cutout, x))), y\n",
        "\n",
        "\tdef _cutout(self, image_origin):\n",
        "\t\t# 最後に使うfill()は元の画像を書き換えるので、コピーしておく\n",
        "\t\timg = np.copy(image_origin)\n",
        "\t\tmask_value = img.mean()\n",
        "\t\t# 乱数固定(flowでseed固定したら必要ないかも)\n",
        "\n",
        "\t\th, w, _ = img.shape\n",
        "\t\t# マスクをかける場所のtop, leftをランダムに決める\n",
        "\t\t# はみ出すことを許すので、0以上ではなく負の値もとる(最大mask_size // 2はみ出す)\n",
        "\t\ttop = np.random.randint(0 - self.cutout_mask_size // 2, h - self.cutout_mask_size)\n",
        "\t\tleft = np.random.randint(0 - self.cutout_mask_size // 2, w - self.cutout_mask_size)\n",
        "\t\tbottom = top + self.cutout_mask_size\n",
        "\t\tright = left + self.cutout_mask_size\n",
        "\n",
        "\t\t# はみ出した場合の処理\n",
        "\t\tif top < 0:\n",
        "\t\t\ttop = 0\n",
        "\t\tif left < 0:\n",
        "\t\t\tleft = 0\n",
        "\n",
        "\t\t# マスク部分の画素値を平均値で埋める\n",
        "\t\timg[top:bottom, left:right, :].fill(mask_value)\n",
        "\t\treturn img\n",
        "\n",
        "\n",
        "\tdef flow(self, \n",
        "\t\t\t x, y=None, \n",
        "\t\t\t batch_size=32, \n",
        "\t\t\t shuffle=True,\n",
        "\t\t\t sample_weight=None,\n",
        "\t\t\t seed=None, \n",
        "\t\t\t save_to_dir=None, \n",
        "\t\t\t save_prefix='', \n",
        "\t\t\t save_format='png', \n",
        "\t\t\t subset=None\n",
        "\t\t):\n",
        "\t\t\n",
        "\t\tbatches = super().flow(x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\n",
        "\t\t# 拡張処理\n",
        "\t\twhile True:\n",
        "\t\t\tbatch_x, batch_y = next(batches)\n",
        "\t\t\t\n",
        "\t\t\t# mix up\n",
        "\t\t\tif self.mix_up_alpha > 0:\n",
        "\t\t\t\twhile True:\n",
        "\t\t\t\t\tbatch_x_2, batch_y_2 = next(batches)\n",
        "\t\t\t\t\tm1, m2 = batch_x.shape[0], batch_x_2.shape[0]\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tif m1 < m2:\n",
        "\t\t\t\t\t\tbatch_x_2 = batch_x_2[:m1]\n",
        "\t\t\t\t\t\tbatch_y_2 = batch_y_2[:m1]\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\telif m1 == m2:\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\tbatch_x, batch_y = self.mix_up(batch_x, batch_y, batch_x_2, batch_y_2)\n",
        "\t\t\t\n",
        "\t\t\t# Random crop\n",
        "\t\t\tif self.random_crop_size is not None:\n",
        "\t\t\t\tx = np.zeros((batch_x.shape[0], self.random_crop_size[0], self.random_crop_size[1], 3))\n",
        "\t\t\t\tfor i in range(batch_x.shape[0]):\n",
        "\t\t\t\t\tx[i] = self.random_crop(batch_x[i])\n",
        "\t\t\t\tbatch_x = x\n",
        "\t\t\t\n",
        "\t\t\tif self.cutout_mask_size > 0:\n",
        "\t\t\t\tbatch_x, batch_y = self.cutout(batch_x, batch_y)\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\t\tyield (batch_x, batch_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TWZvmmxmsK2",
        "colab_type": "text"
      },
      "source": [
        "## Training, Evaluation用クラスの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxj9ZLVbmqnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
        "from keras.optimizers import RMSprop, Adam, Nadam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "from time import gmtime, strftime\n",
        "import os\n",
        "\n",
        "\n",
        "def make_tensorboard(set_dir_name=''):\n",
        "    tictoc = strftime(\"%a_%d_%b_%Y_%H_%M_%S\", gmtime())\n",
        "    directory_name = tictoc\n",
        "    log_dir = set_dir_name + '_' + directory_name\n",
        "    os.mkdir(log_dir)\n",
        "    tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, )\n",
        "    return tensorboard\n",
        "\n",
        "class Trainer():\n",
        "\t\n",
        "\tdef __init__(self, model, loss, optimizer, logdir = './'):\n",
        "\t\tself._target = model\n",
        "\t\tself._target.compile(\n",
        "\t\t\t\tloss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
        "\t\t\t\t)\n",
        "\t\tself.verbose = 1 # visualize progress bar: 0(OFF), 1(On), 2(On:each data) \n",
        "\t\t#self.log_dir = os.path.join(os.path.dirname(__file__), logdir)\n",
        "\t\tself.log_dir = os.path.join(logdir)\n",
        "\t\tself.model_file_name = \"model_file.hdf5\"\n",
        "\t\n",
        "\tdef train_for_tuning_test_data(self, \n",
        "            x_train, y_train, x_test, y_test, batch_size, epochs, lr_scheduler):\n",
        "\t\tdatagen = ImageDataGeneratorEX(\n",
        "\t\t\t      featurewise_center=False,            # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,             # set each sample mean to 0\n",
        "            featurewise_std_normalization=False, # divide inputs by std\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,                 # apply ZCA whitening\n",
        "            rotation_range=0,                   # randomly rotate images in the range (0~180)\n",
        "            width_shift_range=0.0,               # randomly shift images horizontally\n",
        "            height_shift_range=0.0,              # randomly shift images vertically\n",
        "            zoom_range = 0.0,\n",
        "            channel_shift_range = 0.0,\n",
        "            horizontal_flip=True,                # randomly flip images\n",
        "            vertical_flip=False,                 # randomly flip images\n",
        "            random_crop=None,\n",
        "\t\t\t      mix_up_alpha=0.2, \n",
        "\t\t\t      cutout_mask_size=16\n",
        "\t\t)\n",
        "        \n",
        "    # training (validation dataはデータ拡張はしない)\n",
        "\t\tmodel_path = os.path.join(self.log_dir, self.model_file_name)\n",
        "\t\tself._target.fit_generator(\n",
        "            generator        = datagen.flow(x_train,y_train, batch_size),\n",
        "            steps_per_epoch  = x_train.shape[0] // batch_size,\n",
        "            epochs           = epochs,\n",
        "            validation_data  = ImageDataGeneratorEX().flow(x_test,y_test, batch_size),\n",
        "\t\t\t      validation_steps = x_test.shape[0] // batch_size,\n",
        "            callbacks=[\n",
        "                LearningRateScheduler(lr_scheduler),\n",
        "                make_tensorboard(set_dir_name=self.log_dir),\n",
        "                ModelCheckpoint(model_path, save_best_only=True,monitor='val_acc',mode='max')\n",
        "            ],\n",
        "            verbose = self.verbose,\n",
        "            use_multiprocessing=True,\n",
        "            workers = 4\n",
        "        )\n",
        "\t\t\n",
        "\n",
        "class Evaluator():\n",
        "    \n",
        "    def __init__(self, result_file_path=\"./prediction_result.csv\"):\n",
        "        self.result_file_path=\"./prediction_result.csv\"\n",
        "        \n",
        "    def simple_evaluate(self, model, x_test, label):\n",
        "        print(\"start evaluation...\")\n",
        "        score = model.evaluate(x_test, y_test, verbose=1)\n",
        "        print(\"Test loss:\", score[0])\n",
        "        print(\"Test accuracy:\", score[1])\n",
        "        return score\n",
        "    \n",
        "    def tta_evaluate(self, model, x_test, label, batch_size = 2500, tta_epochs = 2):\n",
        "        print(\"batch size (TTA): \"+str(batch_size))\n",
        "        print(\"epochs (TTA): \"+str(tta_epochs))\n",
        "        tta = TTA()\n",
        "        tta_pred = tta.predict(model, x_test, batch_size, epochs = tta_epochs)\n",
        "        print(\"Test accuracy(TTA): \",end = \"\")\n",
        "        print( accuracy_score( np.argmax(tta_pred,axis = 1) , np.argmax(label,axis = 1)))\n",
        "        return tta_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCiJEpaB-HAv",
        "colab_type": "text"
      },
      "source": [
        "## 学習率減衰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4me_PZO-F2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_rate_schedule_for_Adam(epoch):\n",
        "\tlr = 0.001\n",
        "\tif(epoch >= 190): lr = 0.0002 \n",
        "\tif(epoch >= 240): lr = 0.0001\n",
        "\treturn lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKULnAcX2ilW",
        "colab_type": "text"
      },
      "source": [
        "## 実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzvig0oHnqoY",
        "colab_type": "code",
        "outputId": "eac349ac-22ac-4e3a-d770-7159f78b0667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "# create dataset\n",
        "dataset = CIFAR10Dataset()\n",
        "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
        "\n",
        "# create model\n",
        "model = deep_cnn(dataset.image_shape, dataset.num_classes)\n",
        "\n",
        "save_dir='/content/drive/My Drive/Colab Notebooks/Logs/deep_cnn_2/'\n",
        "\n",
        "# train the model\n",
        "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=Adam(), logdir=save_dir)\n",
        "trainer.train_for_tuning_test_data(\n",
        "            x_train, y_train, x_test, y_test, batch_size=500, epochs=250, \n",
        "            lr_scheduler=learning_rate_schedule_for_Adam)\n",
        "\n",
        "\n",
        "# bestなモデルをロードする\n",
        "model = load_model(save_dir+trainer.model_file_name)\n",
        "#model = load_model(save_dir+\"cnn_best_acc_model.hdf5\")\n",
        "\n",
        "# show result\n",
        "evaluator = Evaluator()\n",
        "score = evaluator.simple_evaluate(model, x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "100/100 [==============================] - 59s 587ms/step - loss: 2.0195 - acc: 0.2232 - val_loss: 5.4841 - val_acc: 0.1992\n",
            "Epoch 2/250\n",
            "100/100 [==============================] - 54s 540ms/step - loss: 1.7857 - acc: 0.3412 - val_loss: 2.2136 - val_acc: 0.3008\n",
            "Epoch 3/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 1.6348 - acc: 0.4379 - val_loss: 5.3991 - val_acc: 0.2496\n",
            "Epoch 4/250\n",
            "100/100 [==============================] - 54s 539ms/step - loss: 1.5283 - acc: 0.4925 - val_loss: 1.7313 - val_acc: 0.4136\n",
            "Epoch 5/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 1.4812 - acc: 0.5200 - val_loss: 1.8876 - val_acc: 0.4216\n",
            "Epoch 6/250\n",
            "100/100 [==============================] - 54s 539ms/step - loss: 1.4481 - acc: 0.5333 - val_loss: 1.6088 - val_acc: 0.4792\n",
            "Epoch 7/250\n",
            "100/100 [==============================] - 54s 540ms/step - loss: 1.3900 - acc: 0.5577 - val_loss: 1.8529 - val_acc: 0.4428\n",
            "Epoch 8/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 1.3434 - acc: 0.5833 - val_loss: 1.4342 - val_acc: 0.5276\n",
            "Epoch 9/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 1.3193 - acc: 0.5923 - val_loss: 1.2621 - val_acc: 0.5644\n",
            "Epoch 10/250\n",
            "100/100 [==============================] - 54s 540ms/step - loss: 1.3081 - acc: 0.5987 - val_loss: 1.4967 - val_acc: 0.5164\n",
            "Epoch 11/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 1.2821 - acc: 0.6127 - val_loss: 1.3730 - val_acc: 0.5200\n",
            "Epoch 12/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 1.2498 - acc: 0.6287 - val_loss: 0.9891 - val_acc: 0.6488\n",
            "Epoch 13/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 1.2333 - acc: 0.6336 - val_loss: 1.2666 - val_acc: 0.5696\n",
            "Epoch 14/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 1.2206 - acc: 0.6445 - val_loss: 1.5141 - val_acc: 0.4976\n",
            "Epoch 15/250\n",
            "100/100 [==============================] - 54s 539ms/step - loss: 1.1995 - acc: 0.6523 - val_loss: 0.9409 - val_acc: 0.6756\n",
            "Epoch 16/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.1868 - acc: 0.6550 - val_loss: 0.9039 - val_acc: 0.6836\n",
            "Epoch 17/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.1780 - acc: 0.6619 - val_loss: 1.0806 - val_acc: 0.6384\n",
            "Epoch 18/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.1525 - acc: 0.6711 - val_loss: 0.9038 - val_acc: 0.6976\n",
            "Epoch 19/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 1.1426 - acc: 0.6758 - val_loss: 1.0389 - val_acc: 0.6516\n",
            "Epoch 20/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.1397 - acc: 0.6796 - val_loss: 1.1898 - val_acc: 0.6264\n",
            "Epoch 21/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.1217 - acc: 0.6840 - val_loss: 0.9038 - val_acc: 0.7064\n",
            "Epoch 22/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.1123 - acc: 0.6872 - val_loss: 0.8632 - val_acc: 0.7228\n",
            "Epoch 23/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 1.0867 - acc: 0.7019 - val_loss: 0.9567 - val_acc: 0.6788\n",
            "Epoch 24/250\n",
            "100/100 [==============================] - 54s 535ms/step - loss: 1.1059 - acc: 0.6983 - val_loss: 0.9267 - val_acc: 0.6920\n",
            "Epoch 25/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 1.0693 - acc: 0.7122 - val_loss: 1.1218 - val_acc: 0.6348\n",
            "Epoch 26/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 1.0719 - acc: 0.7132 - val_loss: 0.8222 - val_acc: 0.7352\n",
            "Epoch 27/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 1.0459 - acc: 0.7208 - val_loss: 0.6987 - val_acc: 0.7716\n",
            "Epoch 28/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 1.0664 - acc: 0.7131 - val_loss: 0.8377 - val_acc: 0.7208\n",
            "Epoch 29/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.0303 - acc: 0.7260 - val_loss: 0.8584 - val_acc: 0.7028\n",
            "Epoch 30/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.0394 - acc: 0.7214 - val_loss: 0.8532 - val_acc: 0.7144\n",
            "Epoch 31/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 1.0243 - acc: 0.7315 - val_loss: 0.9064 - val_acc: 0.7140\n",
            "Epoch 32/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.0239 - acc: 0.7289 - val_loss: 0.7000 - val_acc: 0.7684\n",
            "Epoch 33/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.0121 - acc: 0.7351 - val_loss: 0.7620 - val_acc: 0.7516\n",
            "Epoch 34/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.0076 - acc: 0.7398 - val_loss: 0.7015 - val_acc: 0.7724\n",
            "Epoch 35/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.9919 - acc: 0.7433 - val_loss: 0.8070 - val_acc: 0.7368\n",
            "Epoch 36/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.9938 - acc: 0.7443 - val_loss: 0.7239 - val_acc: 0.7576\n",
            "Epoch 37/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.9808 - acc: 0.7498 - val_loss: 1.2921 - val_acc: 0.5888\n",
            "Epoch 38/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9949 - acc: 0.7446 - val_loss: 0.7708 - val_acc: 0.7452\n",
            "Epoch 39/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.9595 - acc: 0.7559 - val_loss: 0.6208 - val_acc: 0.8016\n",
            "Epoch 40/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9758 - acc: 0.7535 - val_loss: 0.6319 - val_acc: 0.7900\n",
            "Epoch 41/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9557 - acc: 0.7583 - val_loss: 0.6055 - val_acc: 0.8164\n",
            "Epoch 42/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.9627 - acc: 0.7561 - val_loss: 0.6695 - val_acc: 0.7784\n",
            "Epoch 43/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9499 - acc: 0.7626 - val_loss: 0.6297 - val_acc: 0.8092\n",
            "Epoch 44/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9529 - acc: 0.7598 - val_loss: 0.6945 - val_acc: 0.7808\n",
            "Epoch 45/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.9538 - acc: 0.7627 - val_loss: 0.5924 - val_acc: 0.8096\n",
            "Epoch 46/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9581 - acc: 0.7598 - val_loss: 0.6069 - val_acc: 0.8060\n",
            "Epoch 47/250\n",
            "100/100 [==============================] - 53s 535ms/step - loss: 0.9257 - acc: 0.7718 - val_loss: 0.6121 - val_acc: 0.8040\n",
            "Epoch 48/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9411 - acc: 0.7635 - val_loss: 0.5818 - val_acc: 0.8052\n",
            "Epoch 49/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.9219 - acc: 0.7726 - val_loss: 0.8242 - val_acc: 0.7472\n",
            "Epoch 50/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.9357 - acc: 0.7711 - val_loss: 0.6132 - val_acc: 0.8008\n",
            "Epoch 51/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9186 - acc: 0.7791 - val_loss: 0.5514 - val_acc: 0.8212\n",
            "Epoch 52/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9278 - acc: 0.7717 - val_loss: 0.8246 - val_acc: 0.7564\n",
            "Epoch 53/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9086 - acc: 0.7791 - val_loss: 0.8493 - val_acc: 0.7580\n",
            "Epoch 54/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9165 - acc: 0.7781 - val_loss: 0.5823 - val_acc: 0.8040\n",
            "Epoch 55/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8822 - acc: 0.7873 - val_loss: 0.5543 - val_acc: 0.8196\n",
            "Epoch 56/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.9053 - acc: 0.7782 - val_loss: 0.5277 - val_acc: 0.8288\n",
            "Epoch 57/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8828 - acc: 0.7874 - val_loss: 0.5514 - val_acc: 0.8280\n",
            "Epoch 58/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8851 - acc: 0.7879 - val_loss: 0.5246 - val_acc: 0.8364\n",
            "Epoch 59/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8847 - acc: 0.7903 - val_loss: 0.5490 - val_acc: 0.8272\n",
            "Epoch 60/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8969 - acc: 0.7793 - val_loss: 0.5360 - val_acc: 0.8312\n",
            "Epoch 61/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8735 - acc: 0.7925 - val_loss: 0.6149 - val_acc: 0.8136\n",
            "Epoch 62/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8805 - acc: 0.7939 - val_loss: 0.5298 - val_acc: 0.8248\n",
            "Epoch 63/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8746 - acc: 0.7948 - val_loss: 0.5622 - val_acc: 0.8212\n",
            "Epoch 64/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8618 - acc: 0.7955 - val_loss: 0.6304 - val_acc: 0.8092\n",
            "Epoch 65/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8629 - acc: 0.7961 - val_loss: 0.5083 - val_acc: 0.8452\n",
            "Epoch 66/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8543 - acc: 0.7990 - val_loss: 0.5681 - val_acc: 0.8232\n",
            "Epoch 67/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.8639 - acc: 0.7991 - val_loss: 0.6039 - val_acc: 0.8196\n",
            "Epoch 68/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8738 - acc: 0.7943 - val_loss: 0.5440 - val_acc: 0.8284\n",
            "Epoch 69/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8341 - acc: 0.8026 - val_loss: 0.4933 - val_acc: 0.8540\n",
            "Epoch 70/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8598 - acc: 0.7985 - val_loss: 0.5646 - val_acc: 0.8260\n",
            "Epoch 71/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8367 - acc: 0.8059 - val_loss: 0.4295 - val_acc: 0.8616\n",
            "Epoch 72/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8380 - acc: 0.8061 - val_loss: 0.4374 - val_acc: 0.8548\n",
            "Epoch 73/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8413 - acc: 0.8047 - val_loss: 0.6055 - val_acc: 0.8096\n",
            "Epoch 74/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8450 - acc: 0.8034 - val_loss: 0.6011 - val_acc: 0.8108\n",
            "Epoch 75/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8267 - acc: 0.8107 - val_loss: 0.4542 - val_acc: 0.8616\n",
            "Epoch 76/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8387 - acc: 0.8074 - val_loss: 0.5050 - val_acc: 0.8412\n",
            "Epoch 77/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8287 - acc: 0.8131 - val_loss: 0.5464 - val_acc: 0.8500\n",
            "Epoch 78/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8329 - acc: 0.8124 - val_loss: 0.5203 - val_acc: 0.8404\n",
            "Epoch 79/250\n",
            "100/100 [==============================] - 54s 535ms/step - loss: 0.8256 - acc: 0.8086 - val_loss: 0.4488 - val_acc: 0.8684\n",
            "Epoch 80/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.8342 - acc: 0.8100 - val_loss: 0.5746 - val_acc: 0.8208\n",
            "Epoch 81/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8280 - acc: 0.8123 - val_loss: 0.4228 - val_acc: 0.8644\n",
            "Epoch 82/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8339 - acc: 0.8123 - val_loss: 0.4355 - val_acc: 0.8636\n",
            "Epoch 83/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8056 - acc: 0.8161 - val_loss: 0.4627 - val_acc: 0.8512\n",
            "Epoch 84/250\n",
            "100/100 [==============================] - 54s 535ms/step - loss: 0.8088 - acc: 0.8190 - val_loss: 0.4867 - val_acc: 0.8484\n",
            "Epoch 85/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8166 - acc: 0.8153 - val_loss: 0.4145 - val_acc: 0.8696\n",
            "Epoch 86/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8121 - acc: 0.8201 - val_loss: 0.5097 - val_acc: 0.8432\n",
            "Epoch 87/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.8071 - acc: 0.8192 - val_loss: 0.5762 - val_acc: 0.8260\n",
            "Epoch 88/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7997 - acc: 0.8230 - val_loss: 0.4435 - val_acc: 0.8652\n",
            "Epoch 89/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.8010 - acc: 0.8230 - val_loss: 0.5472 - val_acc: 0.8384\n",
            "Epoch 90/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8079 - acc: 0.8173 - val_loss: 0.5185 - val_acc: 0.8404\n",
            "Epoch 91/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.8074 - acc: 0.8211 - val_loss: 0.4638 - val_acc: 0.8620\n",
            "Epoch 92/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8034 - acc: 0.8189 - val_loss: 0.4708 - val_acc: 0.8616\n",
            "Epoch 93/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7769 - acc: 0.8300 - val_loss: 0.5103 - val_acc: 0.8452\n",
            "Epoch 94/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8004 - acc: 0.8230 - val_loss: 0.4630 - val_acc: 0.8576\n",
            "Epoch 95/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7787 - acc: 0.8306 - val_loss: 0.4545 - val_acc: 0.8616\n",
            "Epoch 96/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7825 - acc: 0.8270 - val_loss: 0.5029 - val_acc: 0.8420\n",
            "Epoch 97/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7877 - acc: 0.8312 - val_loss: 0.5130 - val_acc: 0.8480\n",
            "Epoch 98/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.8023 - acc: 0.8260 - val_loss: 0.4380 - val_acc: 0.8600\n",
            "Epoch 99/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7889 - acc: 0.8272 - val_loss: 0.4486 - val_acc: 0.8536\n",
            "Epoch 100/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7833 - acc: 0.8325 - val_loss: 0.4298 - val_acc: 0.8724\n",
            "Epoch 101/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7720 - acc: 0.8338 - val_loss: 0.4173 - val_acc: 0.8704\n",
            "Epoch 102/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7769 - acc: 0.8318 - val_loss: 0.4237 - val_acc: 0.8760\n",
            "Epoch 103/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7630 - acc: 0.8338 - val_loss: 0.4649 - val_acc: 0.8596\n",
            "Epoch 104/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7627 - acc: 0.8345 - val_loss: 0.5076 - val_acc: 0.8484\n",
            "Epoch 105/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7668 - acc: 0.8316 - val_loss: 0.4059 - val_acc: 0.8768\n",
            "Epoch 106/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7760 - acc: 0.8328 - val_loss: 0.3772 - val_acc: 0.8924\n",
            "Epoch 107/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7646 - acc: 0.8321 - val_loss: 0.4294 - val_acc: 0.8700\n",
            "Epoch 108/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7640 - acc: 0.8330 - val_loss: 0.7406 - val_acc: 0.8048\n",
            "Epoch 109/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7452 - acc: 0.8415 - val_loss: 0.3875 - val_acc: 0.8828\n",
            "Epoch 110/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7629 - acc: 0.8363 - val_loss: 0.4132 - val_acc: 0.8700\n",
            "Epoch 111/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7440 - acc: 0.8440 - val_loss: 0.4250 - val_acc: 0.8716\n",
            "Epoch 112/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7598 - acc: 0.8386 - val_loss: 0.4387 - val_acc: 0.8700\n",
            "Epoch 113/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.7572 - acc: 0.8389 - val_loss: 0.4266 - val_acc: 0.8704\n",
            "Epoch 114/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7489 - acc: 0.8408 - val_loss: 0.3930 - val_acc: 0.8808\n",
            "Epoch 115/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.7477 - acc: 0.8409 - val_loss: 0.4039 - val_acc: 0.8768\n",
            "Epoch 116/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7663 - acc: 0.8347 - val_loss: 0.4437 - val_acc: 0.8700\n",
            "Epoch 117/250\n",
            "100/100 [==============================] - 54s 535ms/step - loss: 0.7311 - acc: 0.8472 - val_loss: 0.4070 - val_acc: 0.8724\n",
            "Epoch 118/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7558 - acc: 0.8357 - val_loss: 0.4057 - val_acc: 0.8676\n",
            "Epoch 119/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7379 - acc: 0.8416 - val_loss: 0.4046 - val_acc: 0.8780\n",
            "Epoch 120/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.7483 - acc: 0.8412 - val_loss: 0.3885 - val_acc: 0.8864\n",
            "Epoch 121/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7413 - acc: 0.8440 - val_loss: 0.4381 - val_acc: 0.8696\n",
            "Epoch 122/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7438 - acc: 0.8429 - val_loss: 0.4481 - val_acc: 0.8708\n",
            "Epoch 123/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7503 - acc: 0.8431 - val_loss: 0.3603 - val_acc: 0.8848\n",
            "Epoch 124/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7360 - acc: 0.8475 - val_loss: 0.4513 - val_acc: 0.8624\n",
            "Epoch 125/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7424 - acc: 0.8460 - val_loss: 0.3937 - val_acc: 0.8816\n",
            "Epoch 126/250\n",
            "100/100 [==============================] - 54s 535ms/step - loss: 0.7425 - acc: 0.8456 - val_loss: 0.3730 - val_acc: 0.8916\n",
            "Epoch 127/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.7252 - acc: 0.8498 - val_loss: 0.4256 - val_acc: 0.8736\n",
            "Epoch 128/250\n",
            "100/100 [==============================] - 54s 535ms/step - loss: 0.7449 - acc: 0.8434 - val_loss: 0.4521 - val_acc: 0.8696\n",
            "Epoch 129/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7226 - acc: 0.8502 - val_loss: 0.3862 - val_acc: 0.8856\n",
            "Epoch 130/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7388 - acc: 0.8480 - val_loss: 0.4035 - val_acc: 0.8720\n",
            "Epoch 131/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.7221 - acc: 0.8502 - val_loss: 0.3934 - val_acc: 0.8820\n",
            "Epoch 132/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.7339 - acc: 0.8462 - val_loss: 0.4990 - val_acc: 0.8472\n",
            "Epoch 133/250\n",
            "100/100 [==============================] - 54s 535ms/step - loss: 0.7372 - acc: 0.8485 - val_loss: 0.3952 - val_acc: 0.8796\n",
            "Epoch 134/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7228 - acc: 0.8502 - val_loss: 0.3918 - val_acc: 0.8768\n",
            "Epoch 135/250\n",
            "100/100 [==============================] - 54s 535ms/step - loss: 0.7240 - acc: 0.8507 - val_loss: 0.3904 - val_acc: 0.8816\n",
            "Epoch 136/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7356 - acc: 0.8472 - val_loss: 0.3829 - val_acc: 0.8804\n",
            "Epoch 137/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7220 - acc: 0.8522 - val_loss: 0.3744 - val_acc: 0.8912\n",
            "Epoch 138/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7174 - acc: 0.8538 - val_loss: 0.4033 - val_acc: 0.8824\n",
            "Epoch 139/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7134 - acc: 0.8551 - val_loss: 0.4673 - val_acc: 0.8672\n",
            "Epoch 140/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7235 - acc: 0.8526 - val_loss: 0.3986 - val_acc: 0.8908\n",
            "Epoch 141/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7159 - acc: 0.8556 - val_loss: 0.3904 - val_acc: 0.8828\n",
            "Epoch 142/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7146 - acc: 0.8551 - val_loss: 0.3981 - val_acc: 0.8820\n",
            "Epoch 143/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7064 - acc: 0.8593 - val_loss: 0.4017 - val_acc: 0.8896\n",
            "Epoch 144/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7261 - acc: 0.8487 - val_loss: 0.4175 - val_acc: 0.8792\n",
            "Epoch 145/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7009 - acc: 0.8574 - val_loss: 0.4761 - val_acc: 0.8584\n",
            "Epoch 146/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7099 - acc: 0.8550 - val_loss: 0.4077 - val_acc: 0.8836\n",
            "Epoch 147/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7011 - acc: 0.8610 - val_loss: 0.3427 - val_acc: 0.9008\n",
            "Epoch 148/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.7078 - acc: 0.8566 - val_loss: 0.4681 - val_acc: 0.8608\n",
            "Epoch 149/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7075 - acc: 0.8542 - val_loss: 0.3831 - val_acc: 0.8884\n",
            "Epoch 150/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7016 - acc: 0.8601 - val_loss: 0.4044 - val_acc: 0.8856\n",
            "Epoch 151/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6914 - acc: 0.8627 - val_loss: 0.3975 - val_acc: 0.8796\n",
            "Epoch 152/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6919 - acc: 0.8603 - val_loss: 0.4322 - val_acc: 0.8716\n",
            "Epoch 153/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7061 - acc: 0.8602 - val_loss: 0.3668 - val_acc: 0.9060\n",
            "Epoch 154/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6875 - acc: 0.8604 - val_loss: 0.4421 - val_acc: 0.8652\n",
            "Epoch 155/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7139 - acc: 0.8581 - val_loss: 0.3940 - val_acc: 0.8828\n",
            "Epoch 156/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7092 - acc: 0.8570 - val_loss: 0.3522 - val_acc: 0.9032\n",
            "Epoch 157/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6933 - acc: 0.8621 - val_loss: 0.3832 - val_acc: 0.8912\n",
            "Epoch 158/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7091 - acc: 0.8584 - val_loss: 0.3666 - val_acc: 0.8884\n",
            "Epoch 159/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7029 - acc: 0.8597 - val_loss: 0.3996 - val_acc: 0.8860\n",
            "Epoch 160/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7065 - acc: 0.8648 - val_loss: 0.4239 - val_acc: 0.8784\n",
            "Epoch 161/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6918 - acc: 0.8643 - val_loss: 0.4104 - val_acc: 0.8836\n",
            "Epoch 162/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.7089 - acc: 0.8597 - val_loss: 0.4099 - val_acc: 0.8828\n",
            "Epoch 163/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6903 - acc: 0.8667 - val_loss: 0.3602 - val_acc: 0.8912\n",
            "Epoch 164/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6838 - acc: 0.8699 - val_loss: 0.3331 - val_acc: 0.8992\n",
            "Epoch 165/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6871 - acc: 0.8646 - val_loss: 0.3490 - val_acc: 0.8892\n",
            "Epoch 166/250\n",
            "100/100 [==============================] - 54s 539ms/step - loss: 0.6851 - acc: 0.8635 - val_loss: 0.3867 - val_acc: 0.8800\n",
            "Epoch 167/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6958 - acc: 0.8637 - val_loss: 0.4152 - val_acc: 0.8756\n",
            "Epoch 168/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6855 - acc: 0.8659 - val_loss: 0.3921 - val_acc: 0.8864\n",
            "Epoch 169/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6838 - acc: 0.8677 - val_loss: 0.3477 - val_acc: 0.8996\n",
            "Epoch 170/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.7023 - acc: 0.8589 - val_loss: 0.4160 - val_acc: 0.8796\n",
            "Epoch 171/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6763 - acc: 0.8682 - val_loss: 0.3780 - val_acc: 0.8856\n",
            "Epoch 172/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6804 - acc: 0.8696 - val_loss: 0.3587 - val_acc: 0.8920\n",
            "Epoch 173/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6785 - acc: 0.8672 - val_loss: 0.3838 - val_acc: 0.8904\n",
            "Epoch 174/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6882 - acc: 0.8676 - val_loss: 0.3698 - val_acc: 0.8848\n",
            "Epoch 175/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6894 - acc: 0.8653 - val_loss: 0.3777 - val_acc: 0.8852\n",
            "Epoch 176/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6767 - acc: 0.8674 - val_loss: 0.3069 - val_acc: 0.9148\n",
            "Epoch 177/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6757 - acc: 0.8682 - val_loss: 0.3892 - val_acc: 0.8784\n",
            "Epoch 178/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6735 - acc: 0.8685 - val_loss: 0.3262 - val_acc: 0.9064\n",
            "Epoch 179/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6839 - acc: 0.8670 - val_loss: 0.3530 - val_acc: 0.9004\n",
            "Epoch 180/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6725 - acc: 0.8707 - val_loss: 0.3914 - val_acc: 0.8872\n",
            "Epoch 181/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.6696 - acc: 0.8728 - val_loss: 0.3530 - val_acc: 0.9008\n",
            "Epoch 182/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6758 - acc: 0.8699 - val_loss: 0.3738 - val_acc: 0.8888\n",
            "Epoch 183/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.6644 - acc: 0.8740 - val_loss: 0.3782 - val_acc: 0.8976\n",
            "Epoch 184/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6853 - acc: 0.8615 - val_loss: 0.3432 - val_acc: 0.8920\n",
            "Epoch 185/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6767 - acc: 0.8701 - val_loss: 0.4057 - val_acc: 0.8828\n",
            "Epoch 186/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6760 - acc: 0.8702 - val_loss: 0.3338 - val_acc: 0.9020\n",
            "Epoch 187/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6540 - acc: 0.8767 - val_loss: 0.3283 - val_acc: 0.9040\n",
            "Epoch 188/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6583 - acc: 0.8715 - val_loss: 0.3772 - val_acc: 0.8888\n",
            "Epoch 189/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6755 - acc: 0.8704 - val_loss: 0.3407 - val_acc: 0.9032\n",
            "Epoch 190/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6642 - acc: 0.8755 - val_loss: 0.3632 - val_acc: 0.8972\n",
            "Epoch 191/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6553 - acc: 0.8814 - val_loss: 0.2988 - val_acc: 0.9180\n",
            "Epoch 192/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6428 - acc: 0.8834 - val_loss: 0.2838 - val_acc: 0.9192\n",
            "Epoch 193/250\n",
            "100/100 [==============================] - 54s 535ms/step - loss: 0.6275 - acc: 0.8880 - val_loss: 0.2995 - val_acc: 0.9092\n",
            "Epoch 194/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6235 - acc: 0.8885 - val_loss: 0.2976 - val_acc: 0.9152\n",
            "Epoch 195/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.6303 - acc: 0.8896 - val_loss: 0.2595 - val_acc: 0.9232\n",
            "Epoch 196/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6227 - acc: 0.8902 - val_loss: 0.2903 - val_acc: 0.9156\n",
            "Epoch 197/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6087 - acc: 0.8926 - val_loss: 0.2951 - val_acc: 0.9168\n",
            "Epoch 198/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6072 - acc: 0.8940 - val_loss: 0.2689 - val_acc: 0.9212\n",
            "Epoch 199/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6304 - acc: 0.8885 - val_loss: 0.2660 - val_acc: 0.9224\n",
            "Epoch 200/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6107 - acc: 0.8921 - val_loss: 0.3011 - val_acc: 0.9164\n",
            "Epoch 201/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.6176 - acc: 0.8904 - val_loss: 0.2891 - val_acc: 0.9132\n",
            "Epoch 202/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.6193 - acc: 0.8902 - val_loss: 0.2899 - val_acc: 0.9180\n",
            "Epoch 203/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6036 - acc: 0.8933 - val_loss: 0.2637 - val_acc: 0.9244\n",
            "Epoch 204/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.6136 - acc: 0.8938 - val_loss: 0.2718 - val_acc: 0.9196\n",
            "Epoch 205/250\n",
            "100/100 [==============================] - 54s 539ms/step - loss: 0.6136 - acc: 0.8904 - val_loss: 0.2781 - val_acc: 0.9220\n",
            "Epoch 206/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6104 - acc: 0.8925 - val_loss: 0.2730 - val_acc: 0.9164\n",
            "Epoch 207/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6043 - acc: 0.8931 - val_loss: 0.2842 - val_acc: 0.9236\n",
            "Epoch 208/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6170 - acc: 0.8919 - val_loss: 0.2964 - val_acc: 0.9120\n",
            "Epoch 209/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.5993 - acc: 0.8959 - val_loss: 0.2591 - val_acc: 0.9308\n",
            "Epoch 210/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.5998 - acc: 0.8959 - val_loss: 0.2993 - val_acc: 0.9140\n",
            "Epoch 211/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.5910 - acc: 0.8963 - val_loss: 0.2635 - val_acc: 0.9224\n",
            "Epoch 212/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6047 - acc: 0.8931 - val_loss: 0.2896 - val_acc: 0.9192\n",
            "Epoch 213/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6128 - acc: 0.8934 - val_loss: 0.2660 - val_acc: 0.9280\n",
            "Epoch 214/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.6156 - acc: 0.8923 - val_loss: 0.2644 - val_acc: 0.9244\n",
            "Epoch 215/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6094 - acc: 0.8980 - val_loss: 0.3265 - val_acc: 0.9072\n",
            "Epoch 216/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.6101 - acc: 0.8902 - val_loss: 0.2822 - val_acc: 0.9160\n",
            "Epoch 217/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.5930 - acc: 0.8972 - val_loss: 0.2871 - val_acc: 0.9208\n",
            "Epoch 218/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6070 - acc: 0.8931 - val_loss: 0.2722 - val_acc: 0.9240\n",
            "Epoch 219/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6060 - acc: 0.8939 - val_loss: 0.2744 - val_acc: 0.9228\n",
            "Epoch 220/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.5929 - acc: 0.8973 - val_loss: 0.2904 - val_acc: 0.9148\n",
            "Epoch 221/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6005 - acc: 0.8947 - val_loss: 0.2549 - val_acc: 0.9240\n",
            "Epoch 222/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6003 - acc: 0.8979 - val_loss: 0.2932 - val_acc: 0.9176\n",
            "Epoch 223/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.5960 - acc: 0.8982 - val_loss: 0.2781 - val_acc: 0.9176\n",
            "Epoch 224/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6036 - acc: 0.8954 - val_loss: 0.2991 - val_acc: 0.9152\n",
            "Epoch 225/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.6055 - acc: 0.8956 - val_loss: 0.3037 - val_acc: 0.9164\n",
            "Epoch 226/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6087 - acc: 0.8959 - val_loss: 0.2731 - val_acc: 0.9220\n",
            "Epoch 227/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.5895 - acc: 0.8996 - val_loss: 0.2547 - val_acc: 0.9232\n",
            "Epoch 228/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.5993 - acc: 0.8990 - val_loss: 0.2967 - val_acc: 0.9172\n",
            "Epoch 229/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.5982 - acc: 0.8984 - val_loss: 0.2747 - val_acc: 0.9236\n",
            "Epoch 230/250\n",
            "100/100 [==============================] - 54s 539ms/step - loss: 0.5991 - acc: 0.8985 - val_loss: 0.2760 - val_acc: 0.9244\n",
            "Epoch 231/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.6006 - acc: 0.8988 - val_loss: 0.3039 - val_acc: 0.9128\n",
            "Epoch 232/250\n",
            "100/100 [==============================] - 54s 540ms/step - loss: 0.5945 - acc: 0.8996 - val_loss: 0.2760 - val_acc: 0.9248\n",
            "Epoch 233/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.5892 - acc: 0.9017 - val_loss: 0.2686 - val_acc: 0.9252\n",
            "Epoch 234/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.6088 - acc: 0.8942 - val_loss: 0.2891 - val_acc: 0.9192\n",
            "Epoch 235/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.5878 - acc: 0.9003 - val_loss: 0.2663 - val_acc: 0.9196\n",
            "Epoch 236/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.6000 - acc: 0.8958 - val_loss: 0.2958 - val_acc: 0.9184\n",
            "Epoch 237/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.5960 - acc: 0.8992 - val_loss: 0.2752 - val_acc: 0.9200\n",
            "Epoch 238/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.5914 - acc: 0.8985 - val_loss: 0.2807 - val_acc: 0.9188\n",
            "Epoch 239/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.5923 - acc: 0.8990 - val_loss: 0.2681 - val_acc: 0.9220\n",
            "Epoch 240/250\n",
            "100/100 [==============================] - 54s 535ms/step - loss: 0.5885 - acc: 0.9035 - val_loss: 0.3046 - val_acc: 0.9200\n",
            "Epoch 241/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.5925 - acc: 0.8972 - val_loss: 0.2740 - val_acc: 0.9236\n",
            "Epoch 242/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.5930 - acc: 0.8993 - val_loss: 0.2732 - val_acc: 0.9292\n",
            "Epoch 243/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.5854 - acc: 0.9038 - val_loss: 0.2770 - val_acc: 0.9208\n",
            "Epoch 244/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.5880 - acc: 0.9048 - val_loss: 0.2847 - val_acc: 0.9212\n",
            "Epoch 245/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.5820 - acc: 0.9010 - val_loss: 0.2528 - val_acc: 0.9260\n",
            "Epoch 246/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.5852 - acc: 0.9028 - val_loss: 0.2898 - val_acc: 0.9168\n",
            "Epoch 247/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.5868 - acc: 0.9027 - val_loss: 0.2670 - val_acc: 0.9244\n",
            "Epoch 248/250\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 0.5820 - acc: 0.9034 - val_loss: 0.2816 - val_acc: 0.9160\n",
            "Epoch 249/250\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.5881 - acc: 0.9010 - val_loss: 0.3076 - val_acc: 0.9088\n",
            "Epoch 250/250\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 0.5791 - acc: 0.9052 - val_loss: 0.2575 - val_acc: 0.9316\n",
            "start evaluation...\n",
            "10000/10000 [==============================] - 8s 819us/step\n",
            "Test loss: 0.27293910855054854\n",
            "Test accuracy: 0.9231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mywGzeIugMUf",
        "colab_type": "text"
      },
      "source": [
        "##  Test Time Augmentation（TTA）を用いた推論 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkHjWCv1gLi6",
        "colab_type": "code",
        "outputId": "14f04991-47f8-4d2f-c1e5-95ab70f6b9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "class TTA:\n",
        "    \n",
        "    #test_time_augmentation\n",
        "    #batch_sizeは，test_sizeの約数!!!\n",
        "    def predict(self, model, x_test, batch_size ,epochs = 10):\n",
        "        \n",
        "        # Augmentation用generatorによるデータセットの作成\n",
        "        data_flow = self.generator(x_test, batch_size)\n",
        "        \n",
        "        test_size = x_test.shape[0]\n",
        "        pred = np.zeros(shape = (test_size,10), dtype = float)\n",
        "        \n",
        "        step_per_epoch = test_size //batch_size\n",
        "        for epoch in range(epochs):\n",
        "            print( 'epoch: ' + str(epoch+1)+'/'+str(epochs))\n",
        "            for step in range(step_per_epoch):\n",
        "                #print( 'step: ' + str(step+1)+'/'+str(step_per_epoch))\n",
        "                sta = batch_size * step\n",
        "                end = sta + batch_size\n",
        "                tmp_x = data_flow.__next__()\n",
        "                pred[sta:end] += model.predict(tmp_x)        \n",
        "        return pred / epochs\n",
        "    \n",
        "    \n",
        "    def generator(self, x_test,batch_size):\n",
        "        return ImageDataGeneratorEX(\n",
        "                    rotation_range = 10,\n",
        "                    horizontal_flip = True,\n",
        "                    height_shift_range = 0.1,\n",
        "                    width_shift_range = 0.1,\n",
        "                    zoom_range = 0.1,\n",
        "                    channel_shift_range = 0.1,\n",
        "            \t\t\t  #random_crop=None,\n",
        "\t\t\t              #mix_up_alpha=0.2,\n",
        "\t\t\t              #cutout_mask_size=16\n",
        "                ).flow(x_test,batch_size = batch_size,shuffle = False, seed=756) #756 9447\n",
        "\n",
        "      \n",
        "# show result\n",
        "evaluator = Evaluator()\n",
        "score = evaluator.tta_evaluate(model, x_test, y_test, batch_size = 500, tta_epochs = 50)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-24ac21dbe8e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# show result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtta_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtta_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Evaluator' is not defined"
          ]
        }
      ]
    }
  ]
}